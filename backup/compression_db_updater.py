#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
openGauss æ•°æ®åº“ç»Ÿä¸€è°ƒåº¦å™¨ - å¼‚æ­¥æ‰¹é‡æ›´æ–°å‹ç¼©å®Œæˆåçš„æ–‡ä»¶ä¿¡æ¯å’Œå†…å­˜æ•°æ®åº“åŒæ­¥
OpenGauss Database Unified Scheduler - Asynchronous batch update of compressed file information and memory database sync
"""

import asyncio
import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
import time
import json

from utils.scheduler.db_utils import is_opengauss, get_opengauss_connection
from backup.utils import format_bytes

logger = logging.getLogger(__name__)


class OpenGaussDBScheduler:
    """openGauss æ•°æ®åº“ç»Ÿä¸€è°ƒåº¦å™¨ - å¼‚æ­¥æ‰¹é‡æ›´æ–°å‹ç¼©å®Œæˆåçš„æ–‡ä»¶ä¿¡æ¯å’Œå†…å­˜æ•°æ®åº“åŒæ­¥
    
    åŠŸèƒ½ï¼š
    1. æ¥æ”¶å‹ç¼©å®Œæˆçš„æ–‡ä»¶ä¿¡æ¯ï¼ˆé€šè¿‡é˜Ÿåˆ—ï¼‰ï¼Œæ¯3000ä¸ªæ–‡ä»¶æ‰¹é‡æ›´æ–°ä¸€æ¬¡æ•°æ®åº“
    2. æ¥æ”¶å†…å­˜æ•°æ®åº“åŒæ­¥è¯·æ±‚ï¼ˆé€šè¿‡é˜Ÿåˆ—ï¼‰ï¼Œæ¯3000ä¸ªæ–‡ä»¶æ‰¹é‡æ’å…¥ä¸€æ¬¡æ•°æ®åº“
    3. ä½¿ç”¨æ— é™é˜Ÿåˆ—ï¼Œæ”¯æŒå¤šå‹ç¼©çº¿ç¨‹å¹¶å‘æäº¤
    4. ç»Ÿä¸€å¤„ç†äº‹åŠ¡æäº¤å’Œå›æ»šï¼Œé¿å…é•¿äº‹åŠ¡é”è¡¨
    5. å¤„ç†å‹ç¼©å–æ¶ˆåçš„å‰©ä½™æ–‡ä»¶
    """
    
    def __init__(self, backup_set_db_id: int, batch_size: int = 3000):
        """
        Args:
            backup_set_db_id: å¤‡ä»½é›†æ•°æ®åº“ID
            batch_size: æ‰¹é‡æ›´æ–°å¤§å°ï¼ˆé»˜è®¤3000ä¸ªæ–‡ä»¶ï¼‰
        """
        self.backup_set_db_id = backup_set_db_id
        self.batch_size = batch_size
        
        # æ— é™é˜Ÿåˆ—ï¼Œç”¨äºæ¥æ”¶å‹ç¼©å®Œæˆçš„æ–‡ä»¶ä¿¡æ¯
        # æ ¼å¼: ('compression', group_idx, file_paths, chunk_number, compressed_size, original_size)
        # æˆ–: ('sync', file_data_map)
        self.update_queue = asyncio.Queue(maxsize=0)
        
        # å‹ç¼©æ›´æ–°ç¼“å†²åŒº
        self.compression_buffer: List[Tuple[int, List[str], int, int, int]] = []  # (group_idx, file_paths, chunk_number, compressed_size, original_size)
        self.compression_buffer_file_count = 0  # å‹ç¼©ç¼“å†²åŒºä¸­çš„æ–‡ä»¶æ€»æ•°
        
        # å†…å­˜æ•°æ®åº“åŒæ­¥ç¼“å†²åŒº
        self.sync_buffer: List[Tuple] = []  # file_data_map ä¸­çš„é¡¹
        self.sync_buffer_file_count = 0  # åŒæ­¥ç¼“å†²åŒºä¸­çš„æ–‡ä»¶æ€»æ•°
        
        self.buffer_lock = asyncio.Lock()  # ä¿æŠ¤ç¼“å†²åŒºçš„é”
        
        # è¿è¡ŒçŠ¶æ€
        self._running = False
        self._update_task: Optional[asyncio.Task] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_compression_received = 0
        self.total_compression_updated = 0
        self.total_compression_batches = 0
        self.total_sync_received = 0
        self.total_sync_inserted = 0
        self.total_sync_batches = 0
        
        # è¡¨å­˜åœ¨æ€§ç¼“å­˜ï¼ˆé¿å…æ¯æ¬¡æ£€æŸ¥ï¼‰
        self._backup_files_table_exists: Optional[bool] = None
        
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self._running:
            logger.warning("[openGaussè°ƒåº¦å™¨] å·²åœ¨è¿è¡Œä¸­")
            return
        
        if not is_opengauss():
            logger.warning("[openGaussè°ƒåº¦å™¨] é openGauss æ¨¡å¼ï¼Œä¸å¯åŠ¨")
            return
        
        self._running = True
        self._update_task = asyncio.create_task(self._update_loop())
        logger.info(f"[openGaussè°ƒåº¦å™¨] å·²å¯åŠ¨ (backup_set_db_id={self.backup_set_db_id}, batch_size={self.batch_size})")
    
    async def stop(self):
        """åœæ­¢è°ƒåº¦å™¨ï¼Œå¤„ç†å‰©ä½™æ–‡ä»¶"""
        if not self._running:
            return
        
        self._running = False
        
        # ç­‰å¾…é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡å¤„ç†å®Œæˆ
        if self._update_task:
            # å‘é€åœæ­¢ä¿¡å·
            await self.update_queue.put(('stop', None, None, None, None, None))
            
            try:
                # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œæœ€å¤šç­‰å¾…60ç§’
                await asyncio.wait_for(self._update_task, timeout=60.0)
            except asyncio.TimeoutError:
                logger.warning("[openGaussè°ƒåº¦å™¨] åœæ­¢è¶…æ—¶ï¼Œå–æ¶ˆä»»åŠ¡")
                self._update_task.cancel()
                try:
                    await self._update_task
                except asyncio.CancelledError:
                    pass
        
        # å¤„ç†ç¼“å†²åŒºä¸­å‰©ä½™çš„æ–‡ä»¶
        async with self.buffer_lock:
            if self.compression_buffer:
                logger.info(f"[å‹ç¼©DBæ›´æ–°å™¨] å¤„ç†å‰©ä½™å‹ç¼©æ›´æ–° {len(self.compression_buffer)} ä¸ªæ‰¹æ¬¡...")
                await self._flush_compression_buffer()
            if self.sync_buffer:
                logger.info(f"[openGaussåŒæ­¥] å¤„ç†å‰©ä½™å†…å­˜åŒæ­¥ {len(self.sync_buffer)} ä¸ªæ–‡ä»¶...")
                await self._flush_sync_buffer()
        
        logger.info(
            f"[ç»Ÿä¸€è°ƒåº¦å™¨] å·²åœæ­¢ - "
            f"å‹ç¼©: æ¥æ”¶={self.total_compression_received}, æ›´æ–°={self.total_compression_updated}, æ‰¹æ¬¡={self.total_compression_batches}; "
            f"åŒæ­¥: æ¥æ”¶={self.total_sync_received}, æ’å…¥={self.total_sync_inserted}, æ‰¹æ¬¡={self.total_sync_batches}"
        )
    
    async def submit_compressed_files(
        self,
        group_idx: int,
        file_paths: List[str],
        chunk_number: int,
        compressed_size: int,
        original_size: int
    ):
        """æäº¤å‹ç¼©å®Œæˆçš„æ–‡ä»¶ä¿¡æ¯
        
        Args:
            group_idx: æ–‡ä»¶ç»„ç´¢å¼•
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸èƒ½ä¸ºç©ºï¼‰
            chunk_number: å—ç¼–å·
            compressed_size: å‹ç¼©åå¤§å°ï¼ˆæ•´ä¸ªæ–‡ä»¶ç»„çš„æ€»å¤§å°ï¼‰
            original_size: åŸå§‹å¤§å°ï¼ˆæ•´ä¸ªæ–‡ä»¶ç»„çš„æ€»å¤§å°ï¼‰
        """
        # ç©ºåˆ—è¡¨æ£€æŸ¥ï¼šé¿å…æ‰§è¡Œæ— æ„ä¹‰çš„ SQL
        if not file_paths:
            logger.debug(f"[å‹ç¼©DBæ›´æ–°å™¨] æ–‡ä»¶ç»„ #{group_idx} ä¸ºç©ºï¼Œè·³è¿‡æäº¤")
            return
        
        if not self._running:
            logger.warning("[å‹ç¼©DBæ›´æ–°å™¨] è°ƒåº¦å™¨æœªè¿è¡Œï¼Œæ— æ³•æäº¤æ–‡ä»¶ä¿¡æ¯")
            return
        
        try:
            await self.update_queue.put((
                'compression',
                group_idx,
                file_paths,
                chunk_number,
                compressed_size,
                original_size
            ))
            self.total_compression_received += len(file_paths)
            logger.info(
                f"[å‹ç¼©DBæ›´æ–°å™¨] âœ… å·²æäº¤å‹ç¼©æ–‡ä»¶ç»„ #{group_idx}: "
                f"{len(file_paths)} ä¸ªæ–‡ä»¶, chunk_number={chunk_number}, "
                f"å‹ç¼©å¤§å°={format_bytes(compressed_size)}"
            )
        except Exception as e:
            logger.error(f"[openGaussè°ƒåº¦å™¨] æäº¤å‹ç¼©æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
    
    async def submit_sync_files(self, file_data_map: List[Tuple]):
        """æäº¤å†…å­˜æ•°æ®åº“åŒæ­¥è¯·æ±‚
        
        Args:
            file_data_map: æ–‡ä»¶æ•°æ®æ˜ å°„åˆ—è¡¨ [(file_record, data_tuple), ...]
        """
        # ç©ºåˆ—è¡¨æ£€æŸ¥ï¼šé¿å…æ‰§è¡Œæ— æ„ä¹‰çš„ SQL
        if not file_data_map:
            logger.debug("[openGaussåŒæ­¥] åŒæ­¥æ–‡ä»¶åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡æäº¤")
            return
        
        if not self._running:
            logger.warning("[openGaussåŒæ­¥] è°ƒåº¦å™¨æœªè¿è¡Œï¼Œæ— æ³•æäº¤åŒæ­¥è¯·æ±‚")
            return
        
        try:
            await self.update_queue.put(('sync', None, None, None, None, file_data_map))
            self.total_sync_received += len(file_data_map)
            logger.info(
                f"[openGaussåŒæ­¥] âœ… å·²æäº¤å†…å­˜åŒæ­¥è¯·æ±‚: {len(file_data_map)} ä¸ªæ–‡ä»¶"
            )
        except Exception as e:
            logger.error(f"[openGaussè°ƒåº¦å™¨] æäº¤åŒæ­¥è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
    
    async def _update_loop(self):
        """æ›´æ–°å¾ªç¯ - ä»é˜Ÿåˆ—è·å–æ–‡ä»¶ä¿¡æ¯å¹¶æ‰¹é‡æ›´æ–°"""
        logger.info("[ç»Ÿä¸€è°ƒåº¦å™¨] æ›´æ–°å¾ªç¯å·²å¯åŠ¨ï¼ˆå¤„ç†å‹ç¼©æ›´æ–°å’Œå†…å­˜åŒæ­¥ï¼‰")
        
        try:
            while self._running:
                try:
                    # ä»é˜Ÿåˆ—è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆå¸¦è¶…æ—¶ï¼Œé¿å…æ— é™ç­‰å¾…ï¼‰
                    item = await asyncio.wait_for(self.update_queue.get(), timeout=5.0)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åœæ­¢ä¿¡å·
                    if isinstance(item, tuple) and len(item) >= 1 and item[0] == 'stop':
                        logger.debug("[openGaussè°ƒåº¦å™¨] æ”¶åˆ°åœæ­¢ä¿¡å·")
                        break
                    
                    # è§£æä»»åŠ¡ç±»å‹
                    task_type = item[0] if isinstance(item, tuple) and len(item) > 0 else None
                    
                    if task_type == 'compression':
                        # å‹ç¼©æ›´æ–°ä»»åŠ¡ï¼ˆå–æ¶ˆ batch_size é™åˆ¶ï¼šæ¯æ¬¡æ”¶åˆ°å°±ç«‹å³åˆ·ä¸€æ¬¡ï¼‰
                        _, group_idx, file_paths, chunk_number, compressed_size, original_size = item
                        # ç©ºåˆ—è¡¨æ£€æŸ¥
                        if not file_paths:
                            continue
                        
                        file_count = len(file_paths)
                        async with self.buffer_lock:
                            self.compression_buffer.append((group_idx, file_paths, chunk_number, compressed_size, original_size))
                            self.compression_buffer_file_count += file_count
                            
                            logger.debug(
                                f"[å‹ç¼©DBæ›´æ–°å™¨] æ¥æ”¶æ–‡ä»¶ç»„ #{group_idx}: {file_count} ä¸ªæ–‡ä»¶, "
                                f"ç¼“å†²åŒºç´¯è®¡: {self.compression_buffer_file_count}ï¼ˆå·²é…ç½®ä¸ºæ¯æ¬¡ç«‹å³åˆ·æ–°ï¼‰"
                            )
                            
                            # ä¸å†ä¾èµ– batch_size æ¡ä»¶ï¼Œæœ‰æ•°æ®å°±ç«‹å³æ‰¹é‡æ›´æ–°
                            await self._flush_compression_buffer()
                    
                    elif task_type == 'sync':
                        # å†…å­˜æ•°æ®åº“åŒæ­¥ä»»åŠ¡ï¼ˆå–æ¶ˆ batch_size é™åˆ¶ï¼šæ¯æ¬¡æ”¶åˆ°å°±ç«‹å³åˆ·ä¸€æ¬¡ï¼‰
                        _, _, _, _, _, file_data_map = item
                        # ç©ºåˆ—è¡¨æ£€æŸ¥
                        if not file_data_map:
                            continue
                        
                        file_count = len(file_data_map)
                        async with self.buffer_lock:
                            self.sync_buffer.extend(file_data_map)
                            self.sync_buffer_file_count += file_count
                            
                            logger.debug(
                                f"[openGaussåŒæ­¥] æ¥æ”¶åŒæ­¥è¯·æ±‚: {file_count} ä¸ªæ–‡ä»¶, "
                                f"ç¼“å†²åŒºç´¯è®¡: {self.sync_buffer_file_count}ï¼ˆå·²é…ç½®ä¸ºæ¯æ¬¡ç«‹å³åˆ·æ–°ï¼‰"
                            )
                            
                            # ä¸å†ä¾èµ– batch_size æ¡ä»¶ï¼Œæœ‰æ•°æ®å°±ç«‹å³æ‰¹é‡åŒæ­¥
                            await self._flush_sync_buffer()
                    
                except asyncio.TimeoutError:
                    # è¶…æ—¶ï¼šæ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰æ•°æ®éœ€è¦å¤„ç†
                    async with self.buffer_lock:
                        if self.compression_buffer:
                            logger.debug(f"[å‹ç¼©DBæ›´æ–°å™¨] è¶…æ—¶ï¼Œå¤„ç†å‹ç¼©ç¼“å†²åŒºä¸­çš„ {len(self.compression_buffer)} ä¸ªæ‰¹æ¬¡")
                            await self._flush_compression_buffer()
                        if self.sync_buffer:
                            logger.debug(f"[openGaussåŒæ­¥] è¶…æ—¶ï¼Œå¤„ç†åŒæ­¥ç¼“å†²åŒºä¸­çš„ {len(self.sync_buffer)} ä¸ªæ–‡ä»¶")
                            await self._flush_sync_buffer()
                    continue
                except Exception as e:
                    logger.error(f"[openGaussè°ƒåº¦å™¨] æ›´æ–°å¾ªç¯å¼‚å¸¸: {str(e)}", exc_info=True)
                    await asyncio.sleep(1)  # é”™è¯¯åçŸ­æš‚ç­‰å¾…
            
            # å¾ªç¯ç»“æŸå‰ï¼Œå¤„ç†å‰©ä½™çš„ç¼“å†²åŒºæ•°æ®
            async with self.buffer_lock:
                if self.compression_buffer:
                    logger.info(f"[å‹ç¼©DBæ›´æ–°å™¨] å¤„ç†å‰©ä½™å‹ç¼©ç¼“å†²åŒºä¸­çš„ {len(self.compression_buffer)} ä¸ªæ‰¹æ¬¡")
                    await self._flush_compression_buffer()
                if self.sync_buffer:
                    logger.info(f"[openGaussåŒæ­¥] å¤„ç†å‰©ä½™åŒæ­¥ç¼“å†²åŒºä¸­çš„ {len(self.sync_buffer)} ä¸ªæ–‡ä»¶")
                    await self._flush_sync_buffer()
        
        except asyncio.CancelledError:
            logger.debug("[ç»Ÿä¸€è°ƒåº¦å™¨] æ›´æ–°å¾ªç¯è¢«å–æ¶ˆ")
            # å–æ¶ˆæ—¶ä¹Ÿå¤„ç†å‰©ä½™æ•°æ®
            async with self.buffer_lock:
                if self.compression_buffer:
                    logger.info(f"[å‹ç¼©DBæ›´æ–°å™¨] å–æ¶ˆæ—¶å¤„ç†å‰©ä½™å‹ç¼©ç¼“å†²åŒºä¸­çš„ {len(self.compression_buffer)} ä¸ªæ‰¹æ¬¡")
                    await self._flush_compression_buffer()
                if self.sync_buffer:
                    logger.info(f"[openGaussåŒæ­¥] å–æ¶ˆæ—¶å¤„ç†å‰©ä½™åŒæ­¥ç¼“å†²åŒºä¸­çš„ {len(self.sync_buffer)} ä¸ªæ–‡ä»¶")
                    await self._flush_sync_buffer()
        except Exception as e:
            logger.error(f"[ç»Ÿä¸€è°ƒåº¦å™¨] æ›´æ–°å¾ªç¯å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            logger.info("[ç»Ÿä¸€è°ƒåº¦å™¨] æ›´æ–°å¾ªç¯å·²ç»“æŸ")
    
    async def _flush_compression_buffer(self):
        """åˆ·æ–°å‹ç¼©æ›´æ–°ç¼“å†²åŒº - æ‰¹é‡æ›´æ–°æ•°æ®åº“"""
        if not self.compression_buffer:
            return
        
        # æå–è¦å¤„ç†çš„æ‰¹æ¬¡ï¼ˆç´¯ç§¯æ–‡ä»¶æ•°è¾¾åˆ° batch_sizeï¼‰
        batches_to_process = []
        files_to_process = 0
        
        for item in self.compression_buffer:
            group_idx, file_paths, chunk_number, compressed_size, original_size = item
            file_count = len(file_paths)
            
            if files_to_process + file_count <= self.batch_size:
                batches_to_process.append(item)
                files_to_process += file_count
            else:
                # å¦‚æœåŠ ä¸Šè¿™ä¸ªæ‰¹æ¬¡ä¼šè¶…è¿‡ batch_sizeï¼Œåœæ­¢
                break
        
        if not batches_to_process:
            return
        
        # ä»ç¼“å†²åŒºç§»é™¤å·²å¤„ç†çš„æ‰¹æ¬¡
        self.compression_buffer = self.compression_buffer[len(batches_to_process):]
        self.compression_buffer_file_count -= files_to_process
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = 0
        total_compressed_size = 0
        total_original_size = 0
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ–‡ä»¶ä¿¡æ¯
        all_file_updates: Dict[str, Dict] = {}  # {file_path: {chunk_number, compressed_size}}
        
        for group_idx, file_paths, chunk_number, compressed_size, original_size in batches_to_process:
            total_files += len(file_paths)
            total_compressed_size += compressed_size
            total_original_size += original_size
            
            # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„å‹ç¼©å¤§å°ï¼ˆå¹³å‡åˆ†é…ï¼‰
            per_file_compressed_size = compressed_size // len(file_paths) if file_paths else 0
            
            # åˆå¹¶åˆ°æ›´æ–°å­—å…¸ï¼ˆå¦‚æœåŒä¸€ä¸ªæ–‡ä»¶åœ¨å¤šä¸ªæ‰¹æ¬¡ä¸­ï¼Œä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯ï¼‰
            for file_path in file_paths:
                all_file_updates[file_path] = {
                    'chunk_number': chunk_number,
                    'compressed_size': per_file_compressed_size
                }
        
        if not all_file_updates:
            logger.warning("[å‹ç¼©DBæ›´æ–°å™¨] æ²¡æœ‰å‹ç¼©æ–‡ä»¶éœ€è¦æ›´æ–°")
            return
        
        logger.info(
            f"[å‹ç¼©DBæ›´æ–°å™¨] ğŸ“¦ å¼€å§‹æ‰¹é‡æ›´æ–°å‹ç¼©ä¿¡æ¯: {len(batches_to_process)} ä¸ªæ‰¹æ¬¡, "
            f"{len(all_file_updates)} ä¸ªæ–‡ä»¶, "
            f"å‹ç¼©å¤§å°={format_bytes(total_compressed_size)}"
        )
        
        # æ‰¹é‡æ›´æ–°æ•°æ®åº“
        update_start_time = time.time()
        try:
            await self._update_compression_opengauss(all_file_updates)
            
            update_time = time.time() - update_start_time
            self.total_compression_updated += len(all_file_updates)
            self.total_compression_batches += 1
            
            logger.info(
                f"[å‹ç¼©DBæ›´æ–°å™¨] âœ… æ‰¹é‡æ›´æ–°å‹ç¼©ä¿¡æ¯å®Œæˆ: {len(all_file_updates)} ä¸ªæ–‡ä»¶, "
                f"è€—æ—¶={update_time:.2f}ç§’, "
                f"é€Ÿåº¦={len(all_file_updates)/update_time:.1f} æ–‡ä»¶/ç§’"
            )
        
        except Exception as e:
            logger.error(f"[å‹ç¼©DBæ›´æ–°å™¨] âŒ æ‰¹é‡æ›´æ–°å‹ç¼©ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
            # æ›´æ–°å¤±è´¥æ—¶ï¼Œå°†æ‰¹æ¬¡é‡æ–°æ”¾å›ç¼“å†²åŒºï¼ˆé¿å…æ•°æ®ä¸¢å¤±ï¼‰
            async with self.buffer_lock:
                self.compression_buffer = batches_to_process + self.compression_buffer
                self.compression_buffer_file_count += files_to_process
            raise
    
    async def _flush_sync_buffer(self):
        """åˆ·æ–°å†…å­˜æ•°æ®åº“åŒæ­¥ç¼“å†²åŒº - æ‰¹é‡æ’å…¥æ•°æ®åº“"""
        if not self.sync_buffer:
            return
        
        # æå–è¦å¤„ç†çš„æ–‡ä»¶ï¼ˆç´¯ç§¯æ–‡ä»¶æ•°è¾¾åˆ° batch_sizeï¼‰
        files_to_process = []
        files_count = 0
        
        for item in self.sync_buffer:
            file_count = 1  # æ¯ä¸ª item æ˜¯ä¸€ä¸ªæ–‡ä»¶
            if files_count + file_count <= self.batch_size:
                files_to_process.append(item)
                files_count += file_count
            else:
                break
        
        if not files_to_process:
            return
        
        # ä»ç¼“å†²åŒºç§»é™¤å·²å¤„ç†çš„æ–‡ä»¶
        self.sync_buffer = self.sync_buffer[len(files_to_process):]
        self.sync_buffer_file_count -= files_count
        
        logger.info(
            f"[openGaussåŒæ­¥] ğŸ“¥ å¼€å§‹æ‰¹é‡åŒæ­¥å†…å­˜æ•°æ®åº“: {len(files_to_process)} ä¸ªæ–‡ä»¶"
        )
        
        # æ‰¹é‡æ’å…¥æ•°æ®åº“
        sync_start_time = time.time()
        try:
            synced_file_ids = await self._insert_sync_files_opengauss(files_to_process)
            
            sync_time = time.time() - sync_start_time
            self.total_sync_inserted += len(synced_file_ids)
            self.total_sync_batches += 1
            
            logger.info(
                f"[openGaussåŒæ­¥] âœ… æ‰¹é‡åŒæ­¥å†…å­˜æ•°æ®åº“å®Œæˆ: {len(synced_file_ids)} ä¸ªæ–‡ä»¶, "
                f"è€—æ—¶={sync_time:.2f}ç§’, "
                f"é€Ÿåº¦={len(synced_file_ids)/sync_time:.1f} æ–‡ä»¶/ç§’"
            )
        
        except Exception as e:
            logger.error(f"[openGaussåŒæ­¥] âŒ æ‰¹é‡åŒæ­¥å†…å­˜æ•°æ®åº“å¤±è´¥: {str(e)}", exc_info=True)
            # åŒæ­¥å¤±è´¥æ—¶ï¼Œå°†æ–‡ä»¶é‡æ–°æ”¾å›ç¼“å†²åŒºï¼ˆé¿å…æ•°æ®ä¸¢å¤±ï¼‰
            async with self.buffer_lock:
                self.sync_buffer = files_to_process + self.sync_buffer
                self.sync_buffer_file_count += files_count
            raise
    
    async def _update_compression_opengauss(self, file_updates: Dict[str, Dict]):
        """æ›´æ–° openGauss æ•°æ®åº“ - å‹ç¼©ä¿¡æ¯æ›´æ–°
        
        Args:
            file_updates: {file_path: {chunk_number, compressed_size}}
        """
        # ç©ºåˆ—è¡¨æ£€æŸ¥ï¼šé¿å…æ‰§è¡Œæ— æ„ä¹‰çš„ SQL
        if not file_updates:
            return
        
        # å‡†å¤‡æ‰¹é‡æ›´æ–°å‚æ•°
        update_params = []
        for file_path, update_info in file_updates.items():
            update_params.append((
                update_info['chunk_number'],
                update_info['compressed_size'],
                self.backup_set_db_id,
                file_path
            ))
        
        # å¤ç”¨è¿æ¥ï¼Œé¿å…è¿æ¥æ³„æ¼
        async with get_opengauss_connection() as conn:
            actual_conn = None
            try:
                # è·å–å®é™…è¿æ¥å¯¹è±¡ï¼ˆç”¨äºäº‹åŠ¡ç®¡ç†ï¼‰
                actual_conn = conn._conn if hasattr(conn, '_conn') else conn
                
                # æ‰§è¡Œæ‰¹é‡æ›´æ–°
                # æ³¨æ„ï¼šå¦‚æœè®°å½•ä¸å­˜åœ¨ï¼ˆå¯èƒ½è¿˜åœ¨å†…å­˜æ•°æ®åº“ä¸­æœªåŒæ­¥ï¼‰ï¼Œæ›´æ–°ä¼šå¤±è´¥ï¼ˆrowcount = 0ï¼‰
                # è¿™æ˜¯æ­£å¸¸çš„ç«äº‰æ¡ä»¶ï¼Œä¸ä¼šå½±å“æ•°æ®ä¸€è‡´æ€§
                rowcount = await conn.executemany(
                    """
                    UPDATE backup_files
                    SET chunk_number = $1,
                        compressed_size = $2,
                        updated_at = NOW()
                    WHERE backup_set_id = $3
                      AND file_path = $4
                      AND (is_copy_success = TRUE OR is_copy_success IS NULL OR is_copy_success = FALSE)
                    """,
                    update_params
                )
                
                # æ˜¾å¼æäº¤äº‹åŠ¡ï¼ˆopenGauss æ¨¡å¼éœ€è¦æ˜¾å¼æäº¤ï¼‰
                try:
                    await actual_conn.commit()
                    logger.debug(f"[openGaussè°ƒåº¦å™¨] å‹ç¼©æ›´æ–°äº‹åŠ¡å·²æäº¤: {rowcount} ä¸ªæ–‡ä»¶")
                except Exception as commit_err:
                    logger.warning(f"[openGaussè°ƒåº¦å™¨] æäº¤äº‹åŠ¡å¤±è´¥ï¼ˆå¯èƒ½å·²è‡ªåŠ¨æäº¤ï¼‰: {commit_err}")
                
                # éªŒè¯æ›´æ–°ç»“æœ
                if rowcount < len(update_params):
                    missing_count = len(update_params) - rowcount
                    logger.debug(
                        f"[openGaussè°ƒåº¦å™¨] âš ï¸ éƒ¨åˆ†æ–‡ä»¶æœªæ›´æ–°: "
                        f"æœŸæœ›={len(update_params)}, å®é™…={rowcount}, ç¼ºå¤±={missing_count}ã€‚"
                        f"è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ç«äº‰æ¡ä»¶ï¼ˆè®°å½•è¿˜åœ¨å†…å­˜æ•°æ®åº“ä¸­æœªåŒæ­¥ï¼‰ã€‚"
                    )
                
            except Exception as e:
                # å¼‚å¸¸æ—¶æ˜¾å¼å›æ»šï¼Œé¿å…é•¿äº‹åŠ¡é”è¡¨
                if actual_conn and hasattr(actual_conn, 'info'):
                    try:
                        transaction_status = actual_conn.info.transaction_status
                        if transaction_status in (1, 3):  # INTRANS or INERROR
                            await actual_conn.rollback()
                            logger.debug("[openGaussè°ƒåº¦å™¨] å¼‚å¸¸æ—¶äº‹åŠ¡å·²å›æ»š")
                    except Exception as rollback_err:
                        logger.warning(f"[openGaussè°ƒåº¦å™¨] å›æ»šäº‹åŠ¡å¤±è´¥: {str(rollback_err)}")
                raise
    
    async def _insert_sync_files_opengauss(self, file_data_map: List[Tuple]) -> List[int]:
        """æ’å…¥å†…å­˜æ•°æ®åº“åŒæ­¥æ–‡ä»¶åˆ° openGauss
        
        Args:
            file_data_map: æ–‡ä»¶æ•°æ®æ˜ å°„åˆ—è¡¨ [(file_record, data_tuple), ...]
            
        Returns:
            æˆåŠŸæ’å…¥çš„æ–‡ä»¶IDåˆ—è¡¨
        """
        # ç©ºåˆ—è¡¨æ£€æŸ¥ï¼šé¿å…æ‰§è¡Œæ— æ„ä¹‰çš„ SQL
        if not file_data_map:
            return []
        
        synced_file_ids: List[int] = []
        
        # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
        insert_data = []
        for file_record, _ in file_data_map:
            if not file_record:
                continue
            
            file_id = file_record[0]
            backup_set_id = file_record[1]
            
            # éªŒè¯ backup_set_id
            if backup_set_id != self.backup_set_db_id:
                logger.error(
                    f"[openGaussè°ƒåº¦å™¨] âš ï¸ æ–‡ä»¶ backup_set_id={backup_set_id} "
                    f"ä¸è°ƒåº¦å™¨çš„ backup_set_db_id={self.backup_set_db_id} ä¸åŒ¹é…ï¼"
                )
                continue
            
            # æå–æ–‡ä»¶æ•°æ®
            file_path = file_record[2]
            file_name = file_record[3]
            directory_path = file_record[4]
            display_name = file_record[5]
            file_type = file_record[6] or "file"
            file_size = file_record[7] or 0
            compressed_size = file_record[8]
            file_permissions = file_record[9]
            file_owner = file_record[10]
            file_group = file_record[11]
            created_time = self._parse_datetime_from_sqlite(file_record[12])
            modified_time = self._parse_datetime_from_sqlite(file_record[13])
            accessed_time = self._parse_datetime_from_sqlite(file_record[14])
            tape_block_start = file_record[15]
            tape_block_count = file_record[16]
            compressed = bool(file_record[17])
            encrypted = bool(file_record[18])
            checksum = file_record[19]
            is_copy_success = bool(file_record[20])
            copy_status_at = self._parse_datetime_from_sqlite(file_record[21])
            backup_time = self._parse_datetime_from_sqlite(file_record[22])
            chunk_number = file_record[23]
            version = file_record[24]
            file_metadata = file_record[25]
            tags = file_record[26]
            
            # å‡†å¤‡æ’å…¥æ•°æ®å…ƒç»„
            data_tuple = (
                backup_set_id, file_path, file_name,
                directory_path, display_name, file_type,
                file_size, compressed_size, file_permissions,
                file_owner, file_group, created_time,
                modified_time, accessed_time, tape_block_start,
                tape_block_count, compressed, encrypted,
                checksum, is_copy_success, copy_status_at,
                backup_time, chunk_number, version,
                file_metadata, tags
            )
            insert_data.append(data_tuple)
            synced_file_ids.append(file_id)
        
        if not insert_data:
            logger.warning("[openGaussè°ƒåº¦å™¨] æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥æ’å…¥")
            return []
        
        # å¤ç”¨è¿æ¥ï¼Œé¿å…è¿æ¥æ³„æ¼
        async with get_opengauss_connection() as conn:
            actual_conn = None
            try:
                # è·å–å®é™…è¿æ¥å¯¹è±¡ï¼ˆç”¨äºäº‹åŠ¡ç®¡ç†ï¼‰
                actual_conn = conn._conn if hasattr(conn, '_conn') else conn
                
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡æ£€æŸ¥ï¼‰
                if self._backup_files_table_exists is None:
                    try:
                        await conn.fetchrow("SELECT 1 FROM backup_files LIMIT 1")
                        self._backup_files_table_exists = True
                    except Exception as table_check_err:
                        error_msg = str(table_check_err)
                        # å¼‚å¸¸æ—¶æ˜¾å¼å›æ»š
                        if hasattr(actual_conn, 'info'):
                            transaction_status = actual_conn.info.transaction_status
                            if transaction_status in (1, 3):
                                await actual_conn.rollback()
                        
                        if "does not exist" in error_msg.lower() or "relation" in error_msg.lower():
                            self._backup_files_table_exists = False
                            logger.warning("[openGaussè°ƒåº¦å™¨] backup_files è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥")
                            return []
                        else:
                            logger.warning(f"[openGaussè°ƒåº¦å™¨] æ£€æŸ¥è¡¨æ—¶å‡ºé”™: {error_msg}")
                            return []
                
                if self._backup_files_table_exists is False:
                    return []
                
                # æ‰§è¡Œæ‰¹é‡æ’å…¥
                rowcount = await conn.executemany(
                    """
                    INSERT INTO backup_files (
                        backup_set_id, file_path, file_name, directory_path, display_name,
                        file_type, file_size, compressed_size, file_permissions, file_owner,
                        file_group, created_time, modified_time, accessed_time, tape_block_start,
                        tape_block_count, compressed, encrypted, checksum, is_copy_success,
                        copy_status_at, backup_time, chunk_number, version, file_metadata, tags,
                        created_at, updated_at
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                        $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
                        $21, $22, $23, $24, $25::jsonb, $26::jsonb, NOW(), NOW()
                    )
                    """,
                    insert_data
                )
                
                # æ˜¾å¼æäº¤äº‹åŠ¡ï¼ˆopenGauss æ¨¡å¼éœ€è¦æ˜¾å¼æäº¤ï¼‰
                try:
                    await actual_conn.commit()
                    logger.debug(f"[openGaussè°ƒåº¦å™¨] å†…å­˜åŒæ­¥äº‹åŠ¡å·²æäº¤: {rowcount} ä¸ªæ–‡ä»¶")
                except Exception as commit_err:
                    logger.warning(f"[openGaussè°ƒåº¦å™¨] æäº¤äº‹åŠ¡å¤±è´¥ï¼ˆå¯èƒ½å·²è‡ªåŠ¨æäº¤ï¼‰: {commit_err}")
                
                # éªŒè¯æ’å…¥ç»“æœ
                if rowcount != len(insert_data):
                    logger.warning(
                        f"[openGaussè°ƒåº¦å™¨] âš ï¸ éƒ¨åˆ†æ–‡ä»¶æœªæ’å…¥: "
                        f"æœŸæœ›={len(insert_data)}, å®é™…={rowcount}"
                    )
                    if rowcount > 0:
                        synced_file_ids = synced_file_ids[:rowcount]
                    else:
                        synced_file_ids = []
                
            except Exception as e:
                # å¼‚å¸¸æ—¶æ˜¾å¼å›æ»šï¼Œé¿å…é•¿äº‹åŠ¡é”è¡¨
                if actual_conn and hasattr(actual_conn, 'info'):
                    try:
                        transaction_status = actual_conn.info.transaction_status
                        if transaction_status in (1, 3):  # INTRANS or INERROR
                            await actual_conn.rollback()
                            logger.debug("[openGaussè°ƒåº¦å™¨] å¼‚å¸¸æ—¶äº‹åŠ¡å·²å›æ»š")
                    except Exception as rollback_err:
                        logger.warning(f"[openGaussè°ƒåº¦å™¨] å›æ»šäº‹åŠ¡å¤±è´¥: {str(rollback_err)}")
                raise
        
        return synced_file_ids
    
    def _parse_datetime_from_sqlite(self, dt_value) -> Optional[datetime]:
        """å°†SQLiteçš„datetimeå€¼è½¬æ¢ä¸ºPython datetimeå¯¹è±¡"""
        if dt_value is None:
            return None
        if isinstance(dt_value, datetime):
            return dt_value
        if isinstance(dt_value, str):
            try:
                return datetime.fromisoformat(dt_value.replace('Z', '+00:00'))
            except:
                try:
                    return datetime.strptime(dt_value, '%Y-%m-%d %H:%M:%S')
                except:
                    return None
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'compression': {
                'total_received': self.total_compression_received,
                'total_updated': self.total_compression_updated,
                'total_batches': self.total_compression_batches,
                'buffer_size': len(self.compression_buffer),
                'buffer_file_count': self.compression_buffer_file_count
            },
            'sync': {
                'total_received': self.total_sync_received,
                'total_inserted': self.total_sync_inserted,
                'total_batches': self.total_sync_batches,
                'buffer_size': len(self.sync_buffer),
                'buffer_file_count': self.sync_buffer_file_count
            },
            'queue_size': self.update_queue.qsize()
        }


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™ CompressionDBUpdater ä½œä¸ºåˆ«å
CompressionDBUpdater = OpenGaussDBScheduler

