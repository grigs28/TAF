#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†…å­˜æ•°æ®åº“å†™å…¥å™¨ - å®Œå…¨æŒ‰ç…§openGauss BackupFileæ¨¡å‹é‡å†™
Memory Database Writer - Rewritten to match openGauss BackupFile model exactly
"""

import asyncio
import logging
import json
import sqlite3
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import aiosqlite

from utils.scheduler.db_utils import get_opengauss_connection
from utils.datetime_utils import now, format_datetime

logger = logging.getLogger(__name__)


class MemoryDBWriter:
    """å†…å­˜æ•°æ®åº“å†™å…¥å™¨ - ä¸openGauss BackupFileæ¨¡å‹å®Œå…¨ä¸€è‡´"""

    def __init__(self, backup_set_db_id: int,
                 sync_batch_size: int = 5000,           # åŒæ­¥æ‰¹æ¬¡å¤§å°
                 sync_interval: int = 30,                # åŒæ­¥é—´éš”(ç§’)
                 max_memory_files: int = 5000000,        # å†…å­˜ä¸­æœ€å¤§æ–‡ä»¶æ•°ï¼ˆ500ä¸‡ï¼‰
                 checkpoint_interval: int = 300,         # æ£€æŸ¥ç‚¹é—´éš”(ç§’)
                 checkpoint_retention_hours: int = 24,   # æ£€æŸ¥ç‚¹ä¿ç•™æ—¶é—´(å°æ—¶)
                 enable_checkpoint: bool = False):        # æ˜¯å¦å¯ç”¨æ£€æŸ¥ç‚¹ï¼Œé»˜è®¤ä¸å¯ç”¨

        self.backup_set_db_id = backup_set_db_id
        self.sync_batch_size = sync_batch_size
        self.sync_interval = sync_interval
        self.max_memory_files = max_memory_files
        self.checkpoint_interval = checkpoint_interval
        self.checkpoint_retention_hours = checkpoint_retention_hours
        self.enable_checkpoint = enable_checkpoint  # æ˜¯å¦å¯ç”¨æ£€æŸ¥ç‚¹

        # æ£€æŸ¥ç‚¹ç›®å½•ï¼šä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ temp/checkpoints ç›®å½•ï¼ˆä»…åœ¨å¯ç”¨æ£€æŸ¥ç‚¹æ—¶åˆ›å»ºï¼‰
        if self.enable_checkpoint:
            project_root = Path(__file__).parent.parent  # backup -> é¡¹ç›®æ ¹ç›®å½•
            self.checkpoint_dir = project_root / "temp" / "checkpoints"
            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.checkpoint_dir = None

        # å†…å­˜æ•°æ®åº“
        self.memory_db = None
        self.db_connection = None

        # åŒæ­¥ç›¸å…³
        self._is_syncing = False
        self._sync_task = None
        self._checkpoint_task = None
        self._last_sync_time = 0
        self._sync_start_time = 0  # å½“å‰åŒæ­¥å¼€å§‹æ—¶é—´
        self._last_checkpoint_time = 0
        self._last_trigger_time = 0  # é˜²æ­¢é¢‘ç¹è§¦å‘åŒæ­¥
        self._last_file_added_time = time.time()  # è®°å½•æœ€åæ·»åŠ æ–‡ä»¶çš„æ—¶é—´
        self._checkpoint_files = []  # è®°å½•åˆ›å»ºçš„æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ—è¡¨ [(æ–‡ä»¶è·¯å¾„, åˆ›å»ºæ—¶é—´, æœ€å¤§æœªåŒæ­¥æ–‡ä»¶ID), ...]

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'total_files': 0,
            'synced_files': 0,
            'sync_batches': 0,
            'total_time': 0,
            'sync_time': 0,
            'memory_usage': 0
        }

    async def initialize(self):
        """åˆå§‹åŒ–å†…å­˜æ•°æ®åº“å’ŒåŒæ­¥ä»»åŠ¡"""
        # å¯åŠ¨æ—¶æ¸…ç†è¿‡æœŸçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä»…åœ¨å¯ç”¨æ£€æŸ¥ç‚¹æ—¶ï¼‰
        if self.enable_checkpoint:
            await self._cleanup_old_checkpoints_on_startup()
        await self._setup_memory_database()
        await self._start_sync_tasks()
        logger.info(f"å†…å­˜æ•°æ®åº“å†™å…¥å™¨å·²åˆå§‹åŒ– (backup_set_id={self.backup_set_db_id}, checkpoint={self.enable_checkpoint})")

    async def _setup_memory_database(self):
        """è®¾ç½®å†…å­˜æ•°æ®åº“ - å®Œå…¨æŒ‰ç…§openGauss BackupFileæ¨¡å‹"""
        # åˆ›å»ºå†…å­˜æ•°æ®åº“è¿æ¥
        self.db_connection = await aiosqlite.connect(":memory:")
        self.memory_db = self.db_connection

        # åˆ›å»ºè¡¨ç»“æ„ - ä¸openGauss BackupFileæ¨¡å‹å®Œå…¨ä¸€è‡´
        await self._create_tables()

        # å¯ç”¨WALæ¨¡å¼æå‡æ€§èƒ½
        await self.memory_db.execute("PRAGMA journal_mode=WAL")
        await self.memory_db.execute("PRAGMA synchronous=NORMAL")
        await self.memory_db.execute("PRAGMA cache_size=10000")
        await self.memory_db.execute("PRAGMA temp_store=memory")

    async def _create_tables(self):
        """åˆ›å»ºå†…å­˜è¡¨ç»“æ„ - ä¸openGauss BackupFileæ¨¡å‹å­—æ®µå®Œå…¨ä¸€è‡´"""
        # æ–‡ä»¶è¡¨ - ä¸models.backup.BackupFileå®Œå…¨ä¸€è‡´çš„å­—æ®µé¡ºåºå’Œç±»å‹
        await self.memory_db.execute("""
            CREATE TABLE IF NOT EXISTS backup_files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                backup_set_id INTEGER NOT NULL,
                file_path TEXT NOT NULL,
                file_name TEXT NOT NULL,
                directory_path TEXT,
                display_name TEXT,
                file_type TEXT NOT NULL DEFAULT 'file',
                file_size BIGINT NOT NULL,
                compressed_size BIGINT,
                file_permissions TEXT,
                file_owner TEXT,
                file_group TEXT,
                created_time TIMESTAMP,
                modified_time TIMESTAMP,
                accessed_time TIMESTAMP,
                tape_block_start BIGINT,
                tape_block_count INTEGER,
                compressed BOOLEAN DEFAULT FALSE,
                encrypted BOOLEAN DEFAULT FALSE,
                checksum TEXT,
                is_copy_success BOOLEAN DEFAULT FALSE,
                copy_status_at TIMESTAMP,
                backup_time TIMESTAMP NOT NULL,
                chunk_number INTEGER,
                version INTEGER DEFAULT 1,
                file_metadata TEXT,
                tags TEXT,
                synced_to_opengauss BOOLEAN DEFAULT FALSE,
                sync_error TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # åˆ›å»ºç´¢å¼•æå‡æŸ¥è¯¢æ€§èƒ½
        await self.memory_db.execute("CREATE INDEX IF NOT EXISTS idx_backup_files_path ON backup_files(file_path)")
        await self.memory_db.execute("CREATE INDEX IF NOT EXISTS idx_backup_files_synced ON backup_files(synced_to_opengauss)")
        await self.memory_db.execute("CREATE INDEX IF NOT EXISTS idx_backup_files_backup_set ON backup_files(backup_set_id)")

        await self.memory_db.commit()

    async def _start_sync_tasks(self):
        """å¯åŠ¨åŒæ­¥ä»»åŠ¡"""
        # å¯åŠ¨å®šæœŸåŒæ­¥ä»»åŠ¡
        self._sync_task = asyncio.create_task(self._sync_loop())
        logger.info(f"å†…å­˜æ•°æ®åº“åŒæ­¥ä»»åŠ¡å·²å¯åŠ¨ (åŒæ­¥é—´éš”: {self.sync_interval}ç§’, æ‰¹æ¬¡å¤§å°: {self.sync_batch_size})")

        # ä»…åœ¨å¯ç”¨æ£€æŸ¥ç‚¹æ—¶å¯åŠ¨æ£€æŸ¥ç‚¹ä»»åŠ¡
        if self.enable_checkpoint:
            self._checkpoint_task = asyncio.create_task(self._checkpoint_loop())
            logger.info(f"æ£€æŸ¥ç‚¹ä»»åŠ¡å·²å¯åŠ¨ (é—´éš”: {self.checkpoint_interval}ç§’)")
        else:
            self._checkpoint_task = None
            logger.info("æ£€æŸ¥ç‚¹åŠŸèƒ½å·²ç¦ç”¨")

    async def add_file(self, file_info: Dict):
        """æ·»åŠ æ–‡ä»¶åˆ°å†…å­˜æ•°æ®åº“ - æ ¹æ®æ–‡ä»¶æ‰«æå™¨è¾“å‡ºæ­£ç¡®æ˜ å°„ï¼ˆå•ä¸ªæ–‡ä»¶ï¼‰"""
        if not self.memory_db:
            await self.initialize()

        try:
            # å‡†å¤‡æ’å…¥æ•°æ® - æ ¹æ®æ–‡ä»¶æ‰«æå™¨è¾“å‡ºæ ¼å¼æ˜ å°„åˆ°BackupFileæ¨¡å‹
            insert_data = self._prepare_insert_data_from_scanner(file_info)

            # æ’å…¥åˆ°å†…å­˜æ•°æ®åº“ - å­—æ®µé¡ºåºä¸BackupFileæ¨¡å‹ä¸€è‡´
            # æ³¨æ„ï¼šæ˜¾å¼åŒ…å« synced_to_opengauss å’Œ sync_error å­—æ®µï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            # éªŒè¯ backup_set_id æ˜¯å¦æ­£ç¡®
            backup_set_id_in_data = insert_data[0] if insert_data else None
            if backup_set_id_in_data != self.backup_set_db_id:
                logger.error(
                    f"[å†…å­˜æ•°æ®åº“] âš ï¸âš ï¸ é”™è¯¯ï¼šæ–‡ä»¶æ•°æ®çš„ backup_set_id={backup_set_id_in_data} "
                    f"ä¸ MemoryDBWriter çš„ backup_set_db_id={self.backup_set_db_id} ä¸åŒ¹é…ï¼"
                )
            
            await self.memory_db.execute("""
                INSERT INTO backup_files (
                    backup_set_id, file_path, file_name, directory_path, display_name,
                    file_type, file_size, compressed_size, file_permissions, file_owner,
                    file_group, created_time, modified_time, accessed_time, tape_block_start,
                    tape_block_count, compressed, encrypted, checksum, is_copy_success,
                    copy_status_at, backup_time, chunk_number, version, file_metadata, tags,
                    synced_to_opengauss, sync_error
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, insert_data + (False, None))  # æ˜¾å¼è®¾ç½® synced_to_opengauss = FALSE, sync_error = NULL

            await self.memory_db.commit()

            self._stats['total_files'] += 1
            self._last_file_added_time = time.time()  # æ›´æ–°æœ€åæ·»åŠ æ–‡ä»¶æ—¶é—´

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³åŒæ­¥
            await self._check_sync_need()

        except Exception as e:
            file_path = file_info.get('path', 'unknown')
            logger.error(
                f"æ·»åŠ æ–‡ä»¶åˆ°å†…å­˜æ•°æ®åº“å¤±è´¥: {e}, "
                f"æ–‡ä»¶è·¯å¾„: {file_path[:200]}, "
                f"file_infoé”®: {list(file_info.keys())}, "
                f"file_infoå€¼: {dict((k, type(v).__name__ if not isinstance(v, (str, int, bool, type(None))) else v) for k, v in file_info.items())}"
            )
            raise

    async def add_files_batch(self, file_info_list: List[Dict]):
        """æ‰¹é‡æ·»åŠ æ–‡ä»¶åˆ°å†…å­˜æ•°æ®åº“ - ä½¿ç”¨æ‰¹é‡æ’å…¥ä¼˜åŒ–æ€§èƒ½
        
        Args:
            file_info_list: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        if not file_info_list:
            return
        
        if not self.memory_db:
            await self.initialize()

        try:
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            insert_data_list = []
            for file_info in file_info_list:
                try:
                    insert_data = self._prepare_insert_data_from_scanner(file_info)
                    # æ·»åŠ  synced_to_opengauss å’Œ sync_error å­—æ®µ
                    insert_data_list.append(insert_data + (False, None))
                except Exception as e:
                    file_path = file_info.get('path', 'unknown')
                    logger.warning(f"å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®å¤±è´¥: {file_path[:200]}, é”™è¯¯: {str(e)}")
                    continue
            
            if not insert_data_list:
                logger.warning("æ‰¹é‡æ’å…¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥æ’å…¥")
                return
            
            # ä½¿ç”¨ executemany æ‰¹é‡æ’å…¥ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šä¸€æ¬¡æ’å…¥å¤šä¸ªæ–‡ä»¶ï¼Œåªæäº¤ä¸€æ¬¡ï¼‰
            await self.memory_db.executemany("""
                INSERT INTO backup_files (
                    backup_set_id, file_path, file_name, directory_path, display_name,
                    file_type, file_size, compressed_size, file_permissions, file_owner,
                    file_group, created_time, modified_time, accessed_time, tape_block_start,
                    tape_block_count, compressed, encrypted, checksum, is_copy_success,
                    copy_status_at, backup_time, chunk_number, version, file_metadata, tags,
                    synced_to_opengauss, sync_error
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, insert_data_list)

            # åªæäº¤ä¸€æ¬¡ï¼ˆæ‰¹é‡æ’å…¥çš„å…³é”®ä¼˜åŒ–ï¼‰
            await self.memory_db.commit()

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            added_count = len(insert_data_list)
            self._stats['total_files'] += added_count
            self._last_file_added_time = time.time()  # æ›´æ–°æœ€åæ·»åŠ æ–‡ä»¶æ—¶é—´

            # æ•°æ®å†™å…¥ä¿è¯æœºåˆ¶ï¼š
            # 1. executemany æ‰§è¡ŒæˆåŠŸï¼ˆå¦‚æœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
            # 2. commit() æˆåŠŸæäº¤äº‹åŠ¡ï¼ˆå¦‚æœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
            # 3. å¦‚æœä»»ä½•æ­¥éª¤å¤±è´¥ï¼Œå¼‚å¸¸ä¼šè¢«æ•è·å¹¶å‘ä¸ŠæŠ›å‡ºï¼Œæ‰«æå™¨ä¼šå¤„ç†ï¼ˆå›é€€åˆ°é€ä¸ªæ·»åŠ ï¼‰
            # å› æ­¤ï¼Œå¦‚æœæ–¹æ³•æ­£å¸¸è¿”å›ï¼ˆæ²¡æœ‰æŠ›å‡ºå¼‚å¸¸ï¼‰ï¼Œæ•°æ®å·²ç»æˆåŠŸå†™å…¥å¹¶æŒä¹…åŒ–
            logger.debug(f"æ‰¹é‡æ’å…¥å®Œæˆï¼šæˆåŠŸæ’å…¥ {added_count} ä¸ªæ–‡ä»¶åˆ°å†…å­˜æ•°æ®åº“ï¼ˆå·²æäº¤äº‹åŠ¡ï¼‰")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³åŒæ­¥ï¼ˆæ‰¹é‡æ·»åŠ ååªæ£€æŸ¥ä¸€æ¬¡ï¼‰
            await self._check_sync_need()

        except Exception as e:
            logger.error(
                f"æ‰¹é‡æ·»åŠ æ–‡ä»¶åˆ°å†…å­˜æ•°æ®åº“å¤±è´¥: {e}, "
                f"æ–‡ä»¶æ•°é‡: {len(file_info_list)}",
                exc_info=True
            )
            raise

    def _prepare_insert_data_from_scanner(self, file_info: Dict) -> tuple:
        """
        æ ¹æ®æ–‡ä»¶æ‰«æå™¨è¾“å‡ºæ ¼å¼å‡†å¤‡æ’å…¥æ•°æ®
        æ–‡ä»¶æ‰«æå™¨è¾“å‡ºæ ¼å¼:
        {
            'path': str(file_path),
            'name': file_path.name,
            'size': stat.st_size,
            'modified_time': datetime.fromtimestamp(stat.st_mtime),
            'permissions': oct(stat.st_mode)[-3:],
            'is_file': file_path.is_file(),
            'is_dir': file_path.is_dir(),
            'is_symlink': file_path.is_symlink()
        }
        """
        # åŸºæœ¬è·¯å¾„ä¿¡æ¯ - æ¥è‡ªæ–‡ä»¶æ‰«æå™¨
        file_path = file_info.get('path', '')
        file_name = file_info.get('name') or Path(file_path).name

        # ç›®å½•è·¯å¾„
        directory_path = str(Path(file_path).parent) if file_path and Path(file_path).parent != Path(file_path).anchor else None

        # æ˜¾ç¤ºåç§°ï¼ˆæš‚æ—¶ä¸æ–‡ä»¶åç›¸åŒï¼‰
        display_name = file_name

        # æ–‡ä»¶ç±»å‹ - æ ¹æ®æ‰«æå™¨è¾“å‡ºåˆ¤æ–­
        if file_info.get('is_file', True):
            file_type = 'file'
        elif file_info.get('is_dir', False):
            file_type = 'directory'
        elif file_info.get('is_symlink', False):
            file_type = 'symlink'
        else:
            file_type = 'file'

        # æ–‡ä»¶å¤§å° - å…³é”®å­—æ®µï¼ç›´æ¥ä»æ‰«æå™¨çš„sizeå­—æ®µè·å–
        file_size = file_info.get('size', 0) or 0

        # å‹ç¼©å¤§å°ï¼ˆåˆå§‹ä¸ºNoneï¼Œå‹ç¼©æ—¶æ›´æ–°ï¼‰
        compressed_size = None

        # æ–‡ä»¶æƒé™ - æ¥è‡ªæ‰«æå™¨
        file_permissions = file_info.get('permissions')

        # æ–‡ä»¶æ‰€æœ‰è€…å’Œç»„ï¼ˆåˆå§‹ä¸ºNoneï¼ŒLinuxç¯å¢ƒä¸‹å¯æ‰©å±•ï¼‰
        file_owner = None
        file_group = None

        # æ—¶é—´æˆ³å¤„ç† - ä¼˜å…ˆä½¿ç”¨æ‰«æå™¨æä¾›çš„modified_time
        modified_time = file_info.get('modified_time')
        if isinstance(modified_time, datetime):
            modified_time = modified_time.replace(tzinfo=timezone.utc)
        else:
            modified_time = datetime.now(timezone.utc)

        # åˆ›å»ºæ—¶é—´å’Œè®¿é—®æ—¶é—´ï¼ˆæš‚æ—¶ä½¿ç”¨ä¿®æ”¹æ—¶é—´ä½œä¸ºé»˜è®¤å€¼ï¼‰
        created_time = modified_time
        accessed_time = modified_time

        # ç£å¸¦ç›¸å…³ä¿¡æ¯ï¼ˆåˆå§‹ä¸ºNoneï¼Œå‹ç¼©æ—¶æ›´æ–°ï¼‰
        tape_block_start = None
        tape_block_count = None
        compressed = False
        encrypted = False
        checksum = None
        is_copy_success = False
        copy_status_at = None

        # å¤‡ä»½æ—¶é—´
        backup_time = datetime.now(timezone.utc)

        # å…¶ä»–å­—æ®µ
        chunk_number = None
        version = 1

        # å…ƒæ•°æ®ï¼ˆè®°å½•æ‰«ææ—¶ä¿¡æ¯ï¼‰
        file_metadata = json.dumps({
            'scanned_at': datetime.now(timezone.utc).isoformat(),
            'scanner_source': 'file_scanner',
            'original_permissions': file_permissions,
            'file_type_detected': file_info.get('is_file', True)
        })

        # æ ‡ç­¾
        tags = json.dumps({'status': 'scanned'})

        return (
            self.backup_set_db_id,     # backup_set_id
            file_path,                 # file_path
            file_name,                 # file_name
            directory_path,            # directory_path
            display_name,              # display_name
            file_type,                 # file_type
            file_size,                 # file_size - å…³é”®å­—æ®µï¼
            compressed_size,           # compressed_size
            file_permissions,          # file_permissions
            file_owner,                # file_owner
            file_group,                # file_group
            created_time,              # created_time
            modified_time,             # modified_time
            accessed_time,             # accessed_time
            tape_block_start,          # tape_block_start
            tape_block_count,          # tape_block_count
            compressed,                # compressed
            encrypted,                 # encrypted
            checksum,                  # checksum
            is_copy_success,           # is_copy_success
            copy_status_at,            # copy_status_at
            backup_time,               # backup_time
            chunk_number,              # chunk_number
            version,                   # version
            file_metadata,             # file_metadata
            tags                       # tags
        )

    async def _check_sync_need(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥ - å¢åŠ è¶…æ—¶æœºåˆ¶å¤„ç†å‰©ä½™å°‘é‡æ–‡ä»¶"""
        # å¦‚æœåŒæ­¥æ­£åœ¨è¿›è¡Œä¸­ï¼Œç›´æ¥è¿”å›ï¼Œé¿å…é‡å¤è§¦å‘å’Œäº§ç”Ÿå¤§é‡æ—¥å¿—
        if self._is_syncing:
            return
        
        current_time = time.time()
        pending_files = await self._get_pending_sync_count()

        # æ¡ä»¶1ï¼šæ–‡ä»¶æ•°é‡è¾¾åˆ°æ‰¹æ¬¡å¤§å°
        if pending_files >= self.sync_batch_size:
            await self._trigger_sync("batch_size_reached")
            return

        # æ¡ä»¶2ï¼šè¾¾åˆ°åŒæ­¥é—´éš”æ—¶é—´
        if current_time - self._last_sync_time >= self.sync_interval:
            await self._trigger_sync("interval_reached")
            return

        # æ¡ä»¶3ï¼šå†…å­˜ä¸­æ–‡ä»¶è¿‡å¤šï¼Œä¸”æœ‰è¶³å¤Ÿå¾…åŒæ­¥æ–‡ä»¶
        # ä¼˜åŒ–ï¼šåªæœ‰åœ¨å¾…åŒæ­¥æ–‡ä»¶è¶…è¿‡æ‰¹æ¬¡å¤§å°çš„50%æ—¶æ‰è§¦å‘ï¼Œé¿å…é¢‘ç¹åŒæ­¥å°‘é‡æ–‡ä»¶
        memory_threshold = min(self.max_memory_files, self.sync_batch_size * 2)
        if (self._stats['total_files'] >= memory_threshold and
            pending_files >= self.sync_batch_size // 2):
            await self._trigger_sync("memory_limit_reached")
            return

        # æ¡ä»¶4ï¼šè¶…æ—¶æœºåˆ¶ - æ‰«æå®Œæˆä½†æ²¡æœ‰è¾¾åˆ°æ‰¹é‡å¤§å°çš„å‰©ä½™æ–‡ä»¶
        # å¦‚æœè¶…è¿‡60ç§’æ²¡æœ‰æ–°æ–‡ä»¶æ·»åŠ ï¼Œä¸”æœ‰å¾…åŒæ­¥æ–‡ä»¶ï¼Œå¼ºåˆ¶åŒæ­¥
        time_since_last_file = current_time - self._last_file_added_time
        if (time_since_last_file >= 60 and pending_files > 0):
            await self._trigger_sync("scan_completed_timeout")
            return

        # æ¡ä»¶5ï¼šæ£€æŸ¥æ‰«ææ˜¯å¦å¯èƒ½å®Œæˆ - é€šè¿‡å¾…åŒæ­¥æ–‡ä»¶å æ€»æ–‡ä»¶çš„æ¯”ä¾‹åˆ¤æ–­
        if pending_files > 0:
            # å¦‚æœ98%ä»¥ä¸Šçš„æ–‡ä»¶éƒ½å·²åŒæ­¥ï¼Œä¸”è·ç¦»ä¸Šæ¬¡åŒæ­¥è¶…è¿‡30ç§’ï¼Œå¼ºåˆ¶åŒæ­¥å‰©ä½™æ–‡ä»¶
            sync_ratio = (self._stats['synced_files'] / max(1, self._stats['total_files']))
            if (sync_ratio >= 0.98 and
                current_time - self._last_sync_time >= 30):
                await self._trigger_sync("almost_complete")
                return

    async def _get_pending_sync_count(self) -> int:
        """è·å–å¾…åŒæ­¥æ–‡ä»¶æ•°é‡ï¼ˆä»…å½“å‰å¤‡ä»½é›†ï¼‰"""
        async with self.memory_db.execute(
            "SELECT COUNT(*) FROM backup_files WHERE backup_set_id = ? AND synced_to_opengauss = FALSE",
            (self.backup_set_db_id,)
        ) as cursor:
            result = await cursor.fetchone()
            return result[0] if result else 0

    async def _trigger_sync(self, reason: str):
        """è§¦å‘åŒæ­¥ - å¢åŠ é˜²æŠ–åŠ¨æœºåˆ¶ï¼Œå¼‚æ­¥æ‰§è¡Œä¸é˜»å¡æ‰«æçº¿ç¨‹"""
        current_time = time.time()

        # é˜²æŠ–åŠ¨ï¼šé¿å…1ç§’å†…é¢‘ç¹è§¦å‘åŒæ­¥
        if current_time - self._last_trigger_time < 1.0:
            logger.debug(f"åŒæ­¥è§¦å‘è¿‡äºé¢‘ç¹ï¼Œè·³è¿‡ (åŸå› : {reason})")
            return

        if self._is_syncing:
            # é™ä½æ—¥å¿—çº§åˆ«ï¼Œé¿å…åŒæ­¥è¿›è¡Œæ—¶äº§ç”Ÿå¤§é‡é‡å¤æ—¥å¿—
            logger.debug(f"åŒæ­¥å·²åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡è§¦å‘ (åŸå› : {reason})")
            return

        self._last_trigger_time = current_time
        
        # æ£€æŸ¥æ•°æ®åº“ç±»å‹ä»¥æ˜¾ç¤ºæ­£ç¡®çš„æ—¥å¿—
        from utils.scheduler.db_utils import is_opengauss
        db_type = "openGauss" if is_opengauss() else "SQLite"
        logger.info(f"è§¦å‘åŒæ­¥åˆ°{db_type} (åŸå› : {reason})")
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡æ‰§è¡ŒåŒæ­¥ï¼Œä¸é˜»å¡å½“å‰çº¿ç¨‹ï¼ˆæ‰«æçº¿ç¨‹ï¼‰
        # è¿™æ ·æ‰«æå’ŒåŒæ­¥å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼Œäº’ä¸é˜»å¡
        asyncio.create_task(self._sync_to_opengauss(reason))

    async def _sync_loop(self):
        """å®šæœŸåŒæ­¥å¾ªç¯"""
        logger.info("å†…å­˜æ•°æ®åº“åŒæ­¥å¾ªç¯å·²å¯åŠ¨ï¼Œç­‰å¾…åŒæ­¥é—´éš”...")
        while True:
            try:
                await asyncio.sleep(self.sync_interval)
                
                logger.info(f"å®šæœŸåŒæ­¥è§¦å‘ï¼ˆé—´éš”: {self.sync_interval}ç§’ï¼‰")

                if not self._is_syncing:
                    await self._sync_to_opengauss("scheduled")
                else:
                    # è·å–åŒæ­¥ä¿¡æ¯ï¼ˆåªåœ¨è·³è¿‡æ—¶è¾“å‡ºï¼‰
                    pending_count = await self._get_pending_sync_count()
                    total_scanned = self._stats['total_files']
                    total_synced = self._stats['synced_files']
                    # è®¡ç®—å½“å‰åŒæ­¥å·²æŒç»­çš„æ—¶é—´
                    if self._sync_start_time > 0:
                        sync_duration = time.time() - self._sync_start_time
                    else:
                        # å¦‚æœæ²¡æœ‰è®°å½•å¼€å§‹æ—¶é—´ï¼Œä½¿ç”¨ä¸Šæ¬¡å®Œæˆæ—¶é—´ä½œä¸ºå‚è€ƒ
                        sync_duration = time.time() - self._last_sync_time if self._last_sync_time > 0 else 0
                    
                    logger.warning(
                        f"âš ï¸ åŒæ­¥æ­£åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡å®šæœŸåŒæ­¥ - "
                        f"å¾…åŒæ­¥: {pending_count} ä¸ªï¼Œ"
                        f"ç´¯è®¡æ€»æ‰«æ: {total_scanned} ä¸ªï¼Œç´¯è®¡æ€»åŒæ­¥: {total_synced} ä¸ªï¼Œ"
                        f"å½“å‰åŒæ­¥å·²æŒç»­: {sync_duration:.1f}ç§’"
                    )
                    # å¦‚æœåŒæ­¥çŠ¶æ€æŒç»­è¶…è¿‡5åˆ†é’Ÿï¼Œè®°å½•è­¦å‘Šï¼ˆå¯èƒ½æ˜¯å¡ä½äº†ï¼‰
                    if sync_duration > 300:
                        logger.error(
                            f"âš ï¸âš ï¸ è­¦å‘Šï¼šåŒæ­¥çŠ¶æ€å·²æŒç»­ {sync_duration:.1f} ç§’ï¼ˆè¶…è¿‡5åˆ†é’Ÿï¼‰ï¼Œ"
                            f"å¯èƒ½å·²å¡ä½ï¼å¾…åŒæ­¥: {pending_count} ä¸ªæ–‡ä»¶ã€‚"
                            f"å»ºè®®æ£€æŸ¥ SQLite é˜Ÿåˆ—ç®¡ç†å™¨æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚"
                    )

            except asyncio.CancelledError:
                logger.info("å†…å­˜æ•°æ®åº“åŒæ­¥å¾ªç¯è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"åŒæ­¥å¾ªç¯å¼‚å¸¸: {e}", exc_info=True)
                await asyncio.sleep(5)  # é”™è¯¯åçŸ­æš‚ç­‰å¾…

    async def _checkpoint_loop(self):
        """æ£€æŸ¥ç‚¹å¾ªç¯ - æŒä¹…åŒ–ä¿æŠ¤"""
        while True:
            try:
                await asyncio.sleep(self.checkpoint_interval)
                await self._create_checkpoint()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ£€æŸ¥ç‚¹å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(30)  # é”™è¯¯åç­‰å¾…æ›´é•¿æ—¶é—´

    async def _sync_to_opengauss(self, reason: str = "manual"):
        """åŒæ­¥æ–‡ä»¶åˆ°ä¸»æ•°æ®åº“ï¼ˆopenGauss æˆ– SQLiteï¼‰
        
        æ¯æ¬¡åŒæ­¥æ—¶ï¼Œå¾ªç¯å¤„ç†æ‰€æœ‰æœªåŒæ­¥çš„æ–‡ä»¶ï¼Œç›´åˆ°å…¨éƒ¨åŒæ­¥å®Œæˆï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
        """
        if self._is_syncing:
            return

        # æ£€æŸ¥æ•°æ®åº“ç±»å‹
        from utils.scheduler.db_utils import is_opengauss
        if not is_opengauss():
            # SQLite æ¨¡å¼ï¼šä½¿ç”¨é˜Ÿåˆ—åŒæ­¥åˆ°ä¸»æ•°æ®åº“ï¼ˆå†™æ“ä½œä¼˜å…ˆï¼‰
            await self._sync_to_sqlite_via_queue(reason)
            return

        self._is_syncing = True
        self._sync_start_time = time.time()  # è®°å½•åŒæ­¥å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—æŒç»­æ—¶é—´ï¼‰
        sync_start_time = self._sync_start_time
        total_synced_count = 0
        batch_number = 0

        try:
            # è®°å½•åŒæ­¥å¼€å§‹æ—¶çš„å¾…åŒæ­¥æ–‡ä»¶æ•°
            initial_pending_count = await self._get_pending_sync_count()
            if initial_pending_count > 0:
                logger.info(f"[åŒæ­¥å¼€å§‹] å¾…åŒæ­¥æ–‡ä»¶æ•°: {initial_pending_count} ä¸ª (åŸå› : {reason})")
            
            # å¾ªç¯åŒæ­¥ï¼Œç›´åˆ°æ‰€æœ‰æœªåŒæ­¥çš„æ–‡ä»¶éƒ½å¤„ç†å®Œæˆ
            while True:
                # è·å–å¾…åŒæ­¥çš„æ–‡ä»¶æ‰¹æ¬¡ï¼ˆæ¯æ¬¡è·å–ä¸€æ‰¹ï¼‰
                files_to_sync = await self._get_files_to_sync()

                if not files_to_sync:
                    # æ²¡æœ‰æ›´å¤šæ–‡ä»¶éœ€è¦åŒæ­¥
                    if batch_number == 0:
                        logger.info("å†…å­˜æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡ä»¶éœ€è¦åŒæ­¥åˆ°openGauss")
                    break

                batch_number += 1
                logger.debug(f"[æ‰¹æ¬¡ {batch_number}] å¼€å§‹åŒæ­¥ {len(files_to_sync)} ä¸ªæ–‡ä»¶åˆ°openGauss (åŸå› : {reason})")

                # æ‰¹é‡åŒæ­¥åˆ°openGauss
                synced_count, synced_file_ids = await self._batch_sync_to_opengauss(files_to_sync)

                # æ›´æ–°åŒæ­¥çŠ¶æ€ï¼ˆåªæ ‡è®°æˆåŠŸåŒæ­¥çš„æ–‡ä»¶ï¼‰
                if synced_file_ids:
                    await self._mark_files_synced(synced_file_ids)

                # æ›´æ–°ç»Ÿè®¡
                total_synced_count += synced_count
                self._stats['synced_files'] += synced_count
                self._stats['sync_batches'] += 1

                logger.debug(f"[æ‰¹æ¬¡ {batch_number}] âœ… åŒæ­¥å®Œæˆ: {synced_count}/{len(files_to_sync)} ä¸ªæ–‡ä»¶å·²æˆåŠŸåŒæ­¥åˆ°openGauss")
                
                # æ£€æŸ¥å‰©ä½™å¾…åŒæ­¥æ–‡ä»¶æ•°ï¼ˆç”¨äºç¡®è®¤æ˜¯å¦æ‰€æœ‰æ–‡ä»¶éƒ½è¢«åŒæ­¥ï¼‰
                pending_count = await self._get_pending_sync_count()
                if pending_count > 0:
                    logger.debug(f"[æ‰¹æ¬¡ {batch_number}] å†…å­˜æ•°æ®åº“ä¸­è¿˜æœ‰ {pending_count} ä¸ªæ–‡ä»¶å¾…åŒæ­¥ï¼Œå°†åœ¨ä¸‹æ¬¡åŒæ­¥æ—¶å¤„ç†")
                
                # å¦‚æœå½“å‰æ‰¹æ¬¡ä¸­è¿˜æœ‰æœªåŒæ­¥çš„æ–‡ä»¶ï¼Œè®°å½•è­¦å‘Š
                if synced_count < len(files_to_sync):
                    remaining = len(files_to_sync) - synced_count
                    logger.info(f"[æ‰¹æ¬¡ {batch_number}] âš ï¸ è¿˜æœ‰ {remaining} ä¸ªæ–‡ä»¶åŒæ­¥å¤±è´¥ï¼Œå°†åœ¨ä¸‹æ¬¡åŒæ­¥æ—¶é‡è¯•")

            # æ‰€æœ‰æ‰¹æ¬¡åŒæ­¥å®Œæˆ
            if batch_number > 0:
                sync_time = time.time() - sync_start_time
                self._stats['sync_time'] += sync_time
                self._last_sync_time = time.time()
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªåŒæ­¥çš„æ–‡ä»¶
                final_pending_count = await self._get_pending_sync_count()
                
                # è·å–ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
                total_scanned = self._stats['total_files']  # æ€»æ‰«ææ•°ï¼ˆä»ä»»åŠ¡å¼€å§‹åˆ°ç°åœ¨ï¼‰
                total_synced_accumulated = self._stats['synced_files']  # ç´¯è®¡æ€»åŒæ­¥æ•°ï¼ˆä»ä»»åŠ¡å¼€å§‹åˆ°ç°åœ¨ï¼‰
                
                logger.info(
                    f"âœ… å…¨éƒ¨åŒæ­¥å®Œæˆ: å…± {batch_number} ä¸ªæ‰¹æ¬¡ï¼Œæ€»è€—æ—¶ {sync_time:.2f}ç§’ï¼Œ"
                    f"åŒæ­¥å¼€å§‹æ—¶å¾…åŒæ­¥: {initial_pending_count} ä¸ªï¼Œ"
                    f"åŒæ­¥å®Œæˆåå‰©ä½™: {final_pending_count} ä¸ªï¼Œ"
                    f"æœ¬æ¬¡åŒæ­¥: {total_synced_count} ä¸ªï¼Œ"
                    f"ç´¯è®¡æ€»æ‰«æ: {total_scanned} ä¸ªï¼Œ"
                    f"ç´¯è®¡æ€»åŒæ­¥: {total_synced_accumulated} ä¸ª"
                )
                
                # æ£€æŸ¥æ€»æ‰«ææ•°å’Œæ€»åŒæ­¥æ•°æ˜¯å¦ä¸€è‡´
                if total_scanned > 0:
                    sync_ratio = (total_synced_accumulated / total_scanned) * 100
                    if total_synced_accumulated < total_scanned:
                        logger.info(
                            f"âš ï¸ åŒæ­¥è¿›åº¦: {sync_ratio:.1f}% "
                            f"ï¼ˆæ€»æ‰«æ: {total_scanned} ä¸ªï¼Œæ€»åŒæ­¥: {total_synced_accumulated} ä¸ªï¼Œ"
                            f"å¾…åŒæ­¥: {total_scanned - total_synced_accumulated} ä¸ªï¼‰"
                        )
                    elif total_synced_accumulated == total_scanned:
                        logger.info(f"âœ… åŒæ­¥å®Œæˆ: æ€»æ‰«æ {total_scanned} ä¸ªæ–‡ä»¶å·²å…¨éƒ¨åŒæ­¥åˆ°openGaussæ•°æ®åº“")
                    else:
                        logger.warning(
                            f"âš ï¸ å¼‚å¸¸: æ€»åŒæ­¥æ•° ({total_synced_accumulated}) å¤§äºæ€»æ‰«ææ•° ({total_scanned})ï¼Œ"
                            f"å¯èƒ½å­˜åœ¨æ•°æ®ä¸ä¸€è‡´"
                        )
                
                if final_pending_count > 0:
                    # è®¡ç®—æ–°å¢çš„æ–‡ä»¶æ•°ï¼ˆåŒæ­¥è¿‡ç¨‹ä¸­ESæ‰«æå™¨æ·»åŠ çš„æ–°æ–‡ä»¶ï¼‰
                    new_files_during_sync = final_pending_count - (initial_pending_count - total_synced_count)
                    if new_files_during_sync > 0:
                        logger.info(f"ğŸ“Š åŒæ­¥è¿‡ç¨‹ä¸­æ–°å¢äº† {new_files_during_sync} ä¸ªæ–‡ä»¶ï¼ˆESæ‰«æå™¨æŒç»­æ·»åŠ ï¼‰")
                    logger.info(f"âš ï¸ ä»æœ‰ {final_pending_count} ä¸ªæ–‡ä»¶æœªåŒæ­¥ï¼Œå°†åœ¨ä¸‹æ¬¡åŒæ­¥æ—¶é‡è¯•")

        except Exception as e:
            logger.error(f"åŒæ­¥åˆ°openGaussæ•°æ®åº“å¤±è´¥: {e}", exc_info=True)
            # è®°å½•åŒæ­¥é”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'files_to_sync' in locals() and files_to_sync:
                await self._mark_sync_error(files_to_sync, str(e))

        finally:
            self._is_syncing = False
            self._sync_start_time = 0  # é‡ç½®åŒæ­¥å¼€å§‹æ—¶é—´

    async def _get_files_to_sync(self) -> List[Tuple]:
        """è·å–å¾…åŒæ­¥çš„æ–‡ä»¶ - æŒ‰ç…§BackupFileæ¨¡å‹å­—æ®µé¡ºåºï¼ˆä»…å½“å‰å¤‡ä»½é›†ï¼‰"""
        # å…ˆæ£€æŸ¥å†…å­˜æ•°æ®åº“ä¸­æœ‰å¤šå°‘æ–‡ä»¶
        async with self.memory_db.execute("""
            SELECT COUNT(*) FROM backup_files
            WHERE backup_set_id = ? AND synced_to_opengauss = FALSE
        """, (self.backup_set_db_id,)) as cursor:
            pending_count = (await cursor.fetchone())[0]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»– backup_set_id çš„æ–‡ä»¶
        async with self.memory_db.execute("""
            SELECT DISTINCT backup_set_id, COUNT(*) as cnt
            FROM backup_files
            WHERE synced_to_opengauss = FALSE
            GROUP BY backup_set_id
            LIMIT 5
        """) as cursor:
            all_pending = await cursor.fetchall()
        
        if pending_count > 0:
            logger.debug(
                f"[åŒæ­¥] å†…å­˜æ•°æ®åº“ä¸­å¾…åŒæ­¥æ–‡ä»¶: backup_set_id={self.backup_set_db_id}, "
                f"æ•°é‡={pending_count}, å…¶ä»–å¤‡ä»½é›†: {all_pending}"
            )
        
        async with self.memory_db.execute("""
            SELECT id, backup_set_id, file_path, file_name, directory_path, display_name,
                   file_type, file_size, compressed_size, file_permissions, file_owner,
                   file_group, created_time, modified_time, accessed_time, tape_block_start,
                   tape_block_count, compressed, encrypted, checksum, is_copy_success,
                   copy_status_at, backup_time, chunk_number, version, file_metadata, tags
            FROM backup_files
            WHERE backup_set_id = ? AND synced_to_opengauss = FALSE
            ORDER BY id
            LIMIT ?
        """, (self.backup_set_db_id, self.sync_batch_size)) as cursor:
            files = await cursor.fetchall()
            if files:
                # éªŒè¯ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ backup_set_id
                first_file_backup_set_id = files[0][1] if len(files[0]) > 1 else None
                if first_file_backup_set_id != self.backup_set_db_id:
                    logger.error(
                        f"[åŒæ­¥] âš ï¸âš ï¸ é”™è¯¯ï¼šå¾…åŒæ­¥æ–‡ä»¶çš„ backup_set_id={first_file_backup_set_id} "
                        f"ä¸ MemoryDBWriter çš„ backup_set_db_id={self.backup_set_db_id} ä¸åŒ¹é…ï¼"
                    )
            return files

    def _parse_datetime_from_sqlite(self, dt_value) -> datetime:
        """å°†SQLiteçš„datetimeå€¼è½¬æ¢ä¸ºPython datetimeå¯¹è±¡"""
        if dt_value is None:
            return None

        if isinstance(dt_value, datetime):
            return dt_value

        if isinstance(dt_value, str):
            try:
                # SQLiteè¿”å›çš„å­—ç¬¦ä¸²æ ¼å¼ï¼š"2025-04-27 06:04:31.136616+00:00"
                # æˆ– "2025-04-27 06:04:31"
                if '+' in dt_value:
                    # å¤„ç†å¸¦æ—¶åŒºçš„æ ¼å¼
                    return datetime.fromisoformat(dt_value.replace('Z', '+00:00'))
                else:
                    # å¤„ç†ä¸å¸¦æ—¶åŒºçš„æ ¼å¼
                    naive_dt = datetime.fromisoformat(dt_value)
                    return naive_dt.replace(tzinfo=timezone.utc)
            except ValueError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å½“å‰æ—¶é—´
                return datetime.now(timezone.utc)

        # å…¶ä»–æƒ…å†µï¼Œè¿”å›å½“å‰æ—¶é—´
        return datetime.now(timezone.utc)

    async def _batch_sync_to_opengauss(self, files: List[Tuple]) -> Tuple[int, List[int]]:
        """æ‰¹é‡åŒæ­¥åˆ°openGauss - ä½¿ç”¨åŸç”ŸSQLæ‰¹é‡æ’å…¥ï¼Œä¸¥ç¦SQLAlchemyè§£æopenGauss
        
        ä¼˜åŒ–ï¼šä½¿ç”¨ executemany å®ç°çœŸæ­£çš„æ‰¹é‡æ’å…¥ï¼Œå¤§å¹…æå‡æ€§èƒ½
        """
        if not files:
            return 0, []

        # æ£€æŸ¥æ•°æ®åº“ç±»å‹
        from utils.scheduler.db_utils import is_opengauss
        if not is_opengauss():
            synced_file_ids = await self._insert_files_to_sqlite(file_data_map)
            return len(synced_file_ids), synced_file_ids

        logger.debug(f"æ­£åœ¨æ‰¹é‡åŒæ­¥ {len(files)} ä¸ªæ–‡ä»¶åˆ°openGaussæ•°æ®åº“ï¼ˆä½¿ç”¨æ‰¹é‡æ’å…¥ä¼˜åŒ–ï¼‰...")
        
        # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
        insert_data = []
        file_data_map = []  # ä¿å­˜æ–‡ä»¶è®°å½•å’Œæ•°æ®çš„å¯¹åº”å…³ç³» [(file_record, data_tuple), ...]
        failed_files = []  # è®°å½•å¤±è´¥çš„æ–‡ä»¶ç´¢å¼•å’Œé”™è¯¯ä¿¡æ¯
        
        for idx, file_record in enumerate(files):
            try:
                # è½¬æ¢æ•°æ®æ ¼å¼ï¼ŒæŒ‰ç…§å†…å­˜æ•°æ®åº“å­—æ®µé¡ºåºæ˜ å°„åˆ°openGauss
                # file_recordå­—æ®µé¡ºåºï¼šid, backup_set_id, file_path, file_name, directory_path, display_name,
                # file_type, file_size, compressed_size, file_permissions, file_owner,
                # file_group, created_time, modified_time, accessed_time, tape_block_start,
                # tape_block_count, compressed, encrypted, checksum, is_copy_success,
                # copy_status_at, backup_time, chunk_number, version, file_metadata, tags

                # ä¿®å¤datetimeå­—æ®µè½¬æ¢
                backup_set_id = file_record[1]
                file_path = file_record[2]
                file_name = file_record[3]
                directory_path = file_record[4]
                display_name = file_record[5]
                file_type = file_record[6]
                file_size = file_record[7]  # å…³é”®å­—æ®µï¼
                compressed_size = file_record[8]
                file_permissions = file_record[9]
                file_owner = file_record[10]
                file_group = file_record[11]

                # ä¿®å¤ï¼šæ­£ç¡®è½¬æ¢datetimeå­—æ®µ
                created_time = self._parse_datetime_from_sqlite(file_record[12])
                modified_time = self._parse_datetime_from_sqlite(file_record[13])
                accessed_time = self._parse_datetime_from_sqlite(file_record[14])
                copy_status_at = self._parse_datetime_from_sqlite(file_record[21])
                backup_time = self._parse_datetime_from_sqlite(file_record[22])

                tape_block_start = file_record[15]
                tape_block_count = file_record[16]
                compressed = bool(file_record[17])
                encrypted = bool(file_record[18])
                checksum = file_record[19]
                is_copy_success = bool(file_record[20])
                chunk_number = file_record[23]
                version = file_record[24]
                file_metadata = file_record[25]
                tags = file_record[26]

                # æ³¨æ„ï¼šæ•°æ®åº“å­—æ®µå·²æ”¹ä¸º TEXT ç±»å‹ï¼Œæ— é•¿åº¦é™åˆ¶ï¼Œä¸éœ€è¦æˆªæ–­
                # å¦‚æœæ•°æ®åº“è¿ç§»æœªæ‰§è¡Œï¼Œå­—æ®µä»ç„¶æ˜¯ VARCHAR(255)ï¼Œä¼šåœ¨æ’å…¥æ—¶æŠ¥é”™
                # è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦æ‰§è¡Œæ•°æ®åº“è¿ç§»å°†å­—æ®µç±»å‹æ”¹ä¸º TEXT

                # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®å…ƒç»„ï¼ˆæŒ‰ç…§ VALUES å­å¥çš„é¡ºåºï¼‰
                data_tuple = (
                    backup_set_id, file_path, file_name,
                    directory_path, display_name, file_type,
                    file_size, compressed_size, file_permissions,
                    file_owner, file_group, created_time,
                    modified_time, accessed_time, tape_block_start,
                    tape_block_count, compressed, encrypted,
                    checksum, is_copy_success, copy_status_at,
                    backup_time, chunk_number, version,
                    file_metadata, tags
                )
                insert_data.append(data_tuple)
                file_data_map.append((file_record, data_tuple))  # ä¿å­˜å¯¹åº”å…³ç³»

            except Exception as e:
                # æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥ï¼Œè®°å½•é”™è¯¯
                file_path_str = file_record[2] if len(file_record) > 2 else 'unknown'
                logger.error(f"å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®å¤±è´¥ï¼ˆç´¢å¼• {idx}ï¼‰: {e}, æ–‡ä»¶: {file_path_str}")
                failed_files.append((idx, file_path_str, str(e)))
                continue

        if not insert_data:
            logger.info(f"æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥æ‰¹é‡æ’å…¥ï¼Œæ‰€æœ‰ {len(files)} ä¸ªæ–‡ä»¶éƒ½åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥")
            return 0, []

        # æ‰§è¡Œæ‰¹é‡æ’å…¥
        synced_file_ids = []  # æˆåŠŸåŒæ­¥çš„æ–‡ä»¶IDåˆ—è¡¨
        try:
            async with get_opengauss_connection() as conn:
                # ä½¿ç”¨ executemany å®ç°çœŸæ­£çš„æ‰¹é‡æ’å…¥
                # æ³¨æ„ï¼šasyncpg çš„ executemany ä¼šè‡ªåŠ¨å¤„ç†æ‰¹é‡æ’å…¥
                await conn.executemany("""
                    INSERT INTO backup_files (
                        backup_set_id, file_path, file_name, directory_path, display_name,
                        file_type, file_size, compressed_size, file_permissions, file_owner,
                        file_group, created_time, modified_time, accessed_time, tape_block_start,
                        tape_block_count, compressed, encrypted, checksum, is_copy_success,
                        copy_status_at, backup_time, chunk_number, version, file_metadata, tags,
                        created_at, updated_at
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                        $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
                        $21, $22, $23, $24, $25::json, $26::json, NOW(), NOW()
                    )
                """, insert_data)
                
                # æ‰¹é‡æ’å…¥æˆåŠŸï¼Œæ‰€æœ‰æ–‡ä»¶éƒ½å·²åŒæ­¥
                synced_count = len(insert_data)
                # æå–æˆåŠŸåŒæ­¥çš„æ–‡ä»¶IDï¼ˆfile_record[0] æ˜¯æ–‡ä»¶IDï¼‰
                synced_file_ids = [file_record[0] for file_record, _ in file_data_map]
                logger.debug(f"æ‰¹é‡æ’å…¥æˆåŠŸ: {synced_count} ä¸ªæ–‡ä»¶å·²åŒæ­¥åˆ°openGaussæ•°æ®åº“")

        except Exception as e:
            # æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€ä¸ªæ’å…¥ä»¥ç¡®å®šå“ªäº›æ–‡ä»¶å¤±è´¥
            logger.warning(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}ï¼Œå°è¯•é€ä¸ªæ’å…¥ä»¥ç¡®å®šå¤±è´¥çš„æ–‡ä»¶...")
            synced_count, synced_file_ids = await self._fallback_individual_insert(file_data_map, failed_files)
        
        # è®°å½•å¤±è´¥çš„æ–‡ä»¶
        if failed_files:
            logger.warning(f"æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥çš„æ–‡ä»¶æ•°: {len(failed_files)}")
            for idx, file_path_str, error_msg in failed_files[:10]:  # åªè®°å½•å‰10ä¸ª
                logger.debug(f"  å¤±è´¥æ–‡ä»¶ [{idx}]: {file_path_str}, é”™è¯¯: {error_msg}")
            if len(failed_files) > 10:
                logger.debug(f"  ... è¿˜æœ‰ {len(failed_files) - 10} ä¸ªå¤±è´¥æ–‡ä»¶æœªæ˜¾ç¤º")

        return synced_count, synced_file_ids

    async def _fallback_individual_insert(self, file_data_map: List[Tuple], failed_files: List[Tuple]) -> Tuple[int, List[int]]:
        """æ‰¹é‡æ’å…¥å¤±è´¥æ—¶çš„å›é€€æ–¹æ¡ˆï¼šé€ä¸ªæ’å…¥ä»¥ç¡®å®šå¤±è´¥çš„æ–‡ä»¶
        
        Args:
            file_data_map: æ–‡ä»¶è®°å½•å’Œæ•°æ®çš„å¯¹åº”å…³ç³»åˆ—è¡¨ [(file_record, data_tuple), ...]
            failed_files: å¤±è´¥æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºè¿½åŠ æ–°çš„å¤±è´¥è®°å½•
            
        Returns:
            (æˆåŠŸåŒæ­¥çš„æ–‡ä»¶æ•°, æˆåŠŸåŒæ­¥çš„æ–‡ä»¶IDåˆ—è¡¨)
        """
        # æ£€æŸ¥æ•°æ®åº“ç±»å‹
        from utils.scheduler.db_utils import is_opengauss
        if not is_opengauss():
            synced_file_ids = await self._insert_files_to_sqlite(file_data_map)
            return len(synced_file_ids), synced_file_ids

        synced_count = 0
        synced_file_ids = []
        async with get_opengauss_connection() as conn:
            for idx, (file_record, data_tuple) in enumerate(file_data_map):
                try:
                    await conn.execute("""
                        INSERT INTO backup_files (
                            backup_set_id, file_path, file_name, directory_path, display_name,
                            file_type, file_size, compressed_size, file_permissions, file_owner,
                            file_group, created_time, modified_time, accessed_time, tape_block_start,
                            tape_block_count, compressed, encrypted, checksum, is_copy_success,
                            copy_status_at, backup_time, chunk_number, version, file_metadata, tags,
                            created_at, updated_at
                        ) VALUES (
                            $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                            $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
                            $21, $22, $23, $24, $25::json, $26::json, NOW(), NOW()
                        )
                    """, *data_tuple)
                    synced_count += 1
                    # è®°å½•æˆåŠŸåŒæ­¥çš„æ–‡ä»¶IDï¼ˆfile_record[0] æ˜¯æ–‡ä»¶IDï¼‰
                    synced_file_ids.append(file_record[0])
                except Exception as e:
                    file_path_str = file_record[2] if len(file_record) > 2 else 'unknown'
                    logger.error(f"å›é€€é€ä¸ªæ’å…¥å¤±è´¥ï¼ˆç´¢å¼• {idx}ï¼‰: {e}, æ–‡ä»¶: {file_path_str}")
                    # æ³¨æ„ï¼šè¿™é‡Œä¸è®°å½•åŸå§‹ç´¢å¼•ï¼Œå› ä¸º file_data_map ä¸­åªåŒ…å«æˆåŠŸå‡†å¤‡æ•°æ®çš„æ–‡ä»¶
                    failed_files.append((idx, file_path_str, str(e)))
        
        return synced_count, synced_file_ids

    async def _mark_files_synced(self, file_ids: List[int]):
        """æ ‡è®°æ–‡ä»¶å·²åŒæ­¥ï¼ˆä»…å½“å‰å¤‡ä»½é›†ï¼‰"""
        if not file_ids:
            return

        placeholders = ','.join(['?' for _ in file_ids])
        await self.memory_db.execute(
            f"UPDATE backup_files SET synced_to_opengauss = TRUE, sync_error = NULL WHERE backup_set_id = ? AND id IN ({placeholders})",
            [self.backup_set_db_id] + file_ids
        )
        await self.memory_db.commit()
        
        # åŒæ­¥æˆåŠŸåï¼Œæ¸…ç†å·²å®Œå…¨åŒæ­¥çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        await self._cleanup_synced_checkpoints()

    async def _mark_sync_error(self, files: List[Tuple], error_message: str):
        """æ ‡è®°åŒæ­¥é”™è¯¯ï¼ˆä»…å½“å‰å¤‡ä»½é›†ï¼‰"""
        file_ids = [f[0] for f in files]

        placeholders = ','.join(['?' for _ in file_ids])
        await self.memory_db.execute(
            f"UPDATE backup_files SET sync_error = ? WHERE backup_set_id = ? AND id IN ({placeholders})",
            [error_message, self.backup_set_db_id] + file_ids
        )
        await self.memory_db.commit()

    async def _insert_files_to_sqlite(self, file_data_map: List[Tuple]) -> List[int]:
        """å°†æ‰«ææ–‡ä»¶åŒæ­¥åˆ° SQLite ä¸»åº“ï¼ˆè°ƒç”¨æ–¹è´Ÿè´£é˜Ÿåˆ—å’Œä¸²è¡Œæ‰§è¡Œï¼‰"""
        from backup.sqlite_backup_db import insert_backup_files_sqlite

        files_payload: List[Dict] = []
        synced_file_ids: List[int] = []

        for file_record, _ in file_data_map:
            if not file_record:
                continue

            file_id = file_record[0]
            backup_set_id = file_record[1]
            file_path = file_record[2]
            file_name = file_record[3]
            directory_path = file_record[4]
            display_name = file_record[5]
            file_type = file_record[6] or "file"
            file_size = file_record[7] or 0
            compressed_size = file_record[8]
            file_permissions = file_record[9]
            file_owner = file_record[10]
            file_group = file_record[11]
            created_time = self._parse_datetime_from_sqlite(file_record[12])
            modified_time = self._parse_datetime_from_sqlite(file_record[13])
            accessed_time = self._parse_datetime_from_sqlite(file_record[14])
            tape_block_start = file_record[15]
            tape_block_count = file_record[16]
            compressed = bool(file_record[17])
            encrypted = bool(file_record[18])
            checksum = file_record[19]
            is_copy_success = bool(file_record[20])
            copy_status_at = self._parse_datetime_from_sqlite(file_record[21])
            backup_time = self._parse_datetime_from_sqlite(file_record[22])
            chunk_number = file_record[23]
            version = file_record[24]
            file_metadata = file_record[25]
            tags = file_record[26]

            files_payload.append(
                {
                    "backup_set_id": backup_set_id,
                    "file_path": file_path,
                    "file_name": file_name,
                    "directory_path": directory_path,
                    "display_name": display_name,
                    "file_type": file_type,
                    "file_size": file_size,
                    "compressed_size": compressed_size,
                    "file_permissions": file_permissions,
                    "file_owner": file_owner,
                    "file_group": file_group,
                    "created_time": created_time,
                    "modified_time": modified_time,
                    "accessed_time": accessed_time,
                    "tape_block_start": tape_block_start,
                    "tape_block_count": tape_block_count,
                    "compressed": compressed,
                    "encrypted": encrypted,
                    "checksum": checksum,
                    "is_copy_success": is_copy_success,
                    "copy_status_at": copy_status_at,
                    "backup_time": backup_time,
                    "chunk_number": chunk_number,
                    "version": version,
                    "file_metadata": file_metadata,
                    "tags": tags,
                }
            )
            synced_file_ids.append(file_id)

        if files_payload:
            # ç›´æ¥å†™å…¥ SQLiteï¼ˆè°ƒç”¨æ–¹è´Ÿè´£ç¡®ä¿ä¸²è¡Œæ‰§è¡Œï¼Œä¾‹å¦‚é€šè¿‡ sqlite_queue_managerï¼‰
            inserted_ids = await insert_backup_files_sqlite(files_payload)
            # insert_backup_files_sqlite è¿”å›æ•°æ®åº“ä¸­æ–°ç”Ÿæˆçš„è‡ªå¢IDï¼Œä½†æˆ‘ä»¬éœ€è¦å†…å­˜æ•°æ®åº“çš„æ–‡ä»¶ID
            # å› æ­¤ä»ç„¶è¿”å› synced_file_idsï¼ˆå†…å­˜æ•°æ®åº“IDï¼‰ï¼Œç”¨äºæ ‡è®°å†…å­˜æ•°æ®åº“çŠ¶æ€
            if not inserted_ids:
                logger.warning("insert_backup_files_sqlite æœªè¿”å›ä»»ä½• IDï¼Œå¯èƒ½æ‰€æœ‰æ–‡ä»¶å·²å­˜åœ¨")

        return synced_file_ids
    
    async def _sync_to_sqlite_via_queue(self, reason: str = "manual"):
        """é€šè¿‡é˜Ÿåˆ—åŒæ­¥æ–‡ä»¶åˆ° SQLite ä¸»åº“ï¼ˆåŒæ­¥æ“ä½œï¼Œæ™®é€šä¼˜å…ˆçº§ï¼‰"""
        from backup.sqlite_queue_manager import execute_sqlite_sync
        
        if self._is_syncing:
            logger.warning(
                f"[SQLiteåŒæ­¥] åŒæ­¥å·²åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡åŒæ­¥è¯·æ±‚ (åŸå› : {reason})ã€‚"
                f"å¦‚æœæ­¤çŠ¶æ€æŒç»­ï¼Œå¯èƒ½æ˜¯ä¹‹å‰çš„åŒæ­¥æœªæ­£ç¡®å®Œæˆã€‚"
            )
            return
        
        logger.info(f"[SQLiteåŒæ­¥] å¼€å§‹åŒæ­¥ (åŸå› : {reason})ï¼Œè®¾ç½® _is_syncing = True")
        self._is_syncing = True
        self._sync_start_time = time.time()  # è®°å½•åŒæ­¥å¼€å§‹æ—¶é—´
        sync_start_time = self._sync_start_time
        total_synced_count = 0
        batch_number = 0

        try:
            # è®°å½•åŒæ­¥å¼€å§‹æ—¶çš„å¾…åŒæ­¥æ–‡ä»¶æ•°
            initial_pending_count = await self._get_pending_sync_count()
            if initial_pending_count > 0:
                logger.info(f"[SQLiteåŒæ­¥å¼€å§‹] å¾…åŒæ­¥æ–‡ä»¶æ•°: {initial_pending_count} ä¸ª (åŸå› : {reason})")
            else:
                logger.info(f"[SQLiteåŒæ­¥å¼€å§‹] æ²¡æœ‰å¾…åŒæ­¥æ–‡ä»¶ (åŸå› : {reason})")
            
            # å¾ªç¯åŒæ­¥ï¼Œç›´åˆ°æ‰€æœ‰æœªåŒæ­¥çš„æ–‡ä»¶éƒ½å¤„ç†å®Œæˆ
            max_batches = 1000  # é˜²æ­¢æ— é™å¾ªç¯
            while batch_number < max_batches:
                # è·å–å¾…åŒæ­¥çš„æ–‡ä»¶æ‰¹æ¬¡ï¼ˆæ¯æ¬¡è·å–ä¸€æ‰¹ï¼‰
                files_to_sync = await self._get_files_to_sync()

                if not files_to_sync:
                    # æ²¡æœ‰æ›´å¤šæ–‡ä»¶éœ€è¦åŒæ­¥
                    if batch_number == 0:
                        logger.info("å†…å­˜æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡ä»¶éœ€è¦åŒæ­¥åˆ°SQLite")
                    break

                batch_number += 1
                logger.debug(
                    f"[SQLiteæ‰¹æ¬¡ {batch_number}] å¼€å§‹åŒæ­¥ {len(files_to_sync)} ä¸ªæ–‡ä»¶ "
                    f"(åŸå› : {reason}, backup_set_db_id={self.backup_set_db_id})"
                )

                # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
                file_data_map = []
                for file_record in files_to_sync:
                    file_data_map.append((file_record, None))  # ç¬¬äºŒä¸ªå‚æ•°åœ¨ SQLite æ¨¡å¼ä¸‹ä¸éœ€è¦

                # é€šè¿‡é˜Ÿåˆ—åŒæ­¥åˆ° SQLiteï¼ˆåŒæ­¥æ“ä½œï¼Œæ™®é€šä¼˜å…ˆçº§ï¼‰
                # _insert_files_to_sqlite è¿”å›å†…å­˜æ•°æ®åº“ä¸­çš„æ–‡ä»¶IDåˆ—è¡¨
                batch_sync_start = time.time()
                try:
                    synced_file_ids = await execute_sqlite_sync(self._insert_files_to_sqlite, file_data_map)
                    batch_sync_time = time.time() - batch_sync_start
                except Exception as batch_error:
                    batch_sync_time = time.time() - batch_sync_start
                    logger.error(
                        f"[SQLiteæ‰¹æ¬¡ {batch_number}] åŒæ­¥å¤±è´¥: {str(batch_error)}ï¼Œ"
                        f"è€—æ—¶: {batch_sync_time:.2f}ç§’",
                        exc_info=True
                    )
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹ï¼Œä¸ä¸­æ–­æ•´ä¸ªåŒæ­¥æµç¨‹
                    continue

                # æ›´æ–°åŒæ­¥çŠ¶æ€ï¼ˆåªæ ‡è®°æˆåŠŸåŒæ­¥çš„æ–‡ä»¶ï¼‰
                if synced_file_ids:
                    await self._mark_files_synced(synced_file_ids)

                # æ›´æ–°ç»Ÿè®¡
                synced_count = len(synced_file_ids)
                total_synced_count += synced_count
                self._stats['synced_files'] += synced_count
                self._stats['sync_batches'] += 1

                logger.info(
                    f"[SQLiteæ‰¹æ¬¡ {batch_number}] âœ… åŒæ­¥å®Œæˆ: {synced_count}/{len(files_to_sync)} ä¸ªæ–‡ä»¶å·²æˆåŠŸåŒæ­¥ï¼Œ"
                    f"è€—æ—¶: {batch_sync_time:.2f}ç§’"
                )

            if batch_number >= max_batches:
                logger.warning(
                    f"[SQLiteåŒæ­¥] è¾¾åˆ°æœ€å¤§æ‰¹æ¬¡é™åˆ¶ ({max_batches})ï¼Œåœæ­¢åŒæ­¥ã€‚"
                    f"å¯èƒ½è¿˜æœ‰æ–‡ä»¶æœªåŒæ­¥ï¼Œå°†åœ¨ä¸‹æ¬¡åŒæ­¥æ—¶ç»§ç»­ã€‚"
                )

            # æ‰€æœ‰æ‰¹æ¬¡åŒæ­¥å®Œæˆ
            if batch_number > 0:
                sync_time = time.time() - sync_start_time
                self._stats['sync_time'] += sync_time
                self._last_sync_time = time.time()
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªåŒæ­¥çš„æ–‡ä»¶
                final_pending_count = await self._get_pending_sync_count()
                
                # è·å–ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
                total_scanned = self._stats['total_files']
                total_synced_accumulated = self._stats['synced_files']
                
                logger.info(
                    f"âœ… SQLiteåŒæ­¥å®Œæˆ: å…± {batch_number} ä¸ªæ‰¹æ¬¡ï¼Œæ€»è€—æ—¶ {sync_time:.2f}ç§’ï¼Œ"
                    f"åŒæ­¥å¼€å§‹æ—¶å¾…åŒæ­¥: {initial_pending_count} ä¸ªï¼Œ"
                    f"åŒæ­¥å®Œæˆåå‰©ä½™: {final_pending_count} ä¸ªï¼Œ"
                    f"æœ¬æ¬¡åŒæ­¥: {total_synced_count} ä¸ªï¼Œ"
                    f"ç´¯è®¡æ€»æ‰«æ: {total_scanned} ä¸ªï¼Œ"
                    f"ç´¯è®¡æ€»åŒæ­¥: {total_synced_accumulated} ä¸ª"
                )

        except Exception as e:
            logger.error(f"[SQLiteåŒæ­¥] åŒæ­¥è¿‡ç¨‹å¼‚å¸¸: {e}", exc_info=True)
        finally:
            logger.info(f"[SQLiteåŒæ­¥] åŒæ­¥ç»“æŸï¼Œè®¾ç½® _is_syncing = False")
            self._is_syncing = False
            self._sync_start_time = 0  # é‡ç½®åŒæ­¥å¼€å§‹æ—¶é—´

    async def _create_checkpoint(self):
        """åˆ›å»ºæ£€æŸ¥ç‚¹ - æŒä¹…åŒ–ä¿æŠ¤"""
        if not self.enable_checkpoint:
            return  # æ£€æŸ¥ç‚¹åŠŸèƒ½å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        
        try:
            # ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
            if self.checkpoint_dir:
                self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
            
            # è·å–åˆ›å»ºæ£€æŸ¥ç‚¹æ—¶çš„æœ€å¤§æœªåŒæ­¥æ–‡ä»¶ID
            max_unsynced_id = await self._get_max_unsynced_file_id()
            
            # åœ¨é¡¹ç›®ç›®å½•ä¸‹çš„ temp/checkpoints ç›®å½•ä¸­åˆ›å»ºæ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_filename = f"tmp{int(time.time() * 1000)}.sql"
            checkpoint_file = str(self.checkpoint_dir / checkpoint_filename)

            # å¤‡ä»½å†…å­˜æ•°æ®åº“
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                async for line in self.memory_db.iterdump():
                    f.write(f"{line}\n")

            self._last_checkpoint_time = time.time()
            # è®°å½•æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼š(æ–‡ä»¶è·¯å¾„, åˆ›å»ºæ—¶é—´, æœ€å¤§æœªåŒæ­¥æ–‡ä»¶ID)
            self._checkpoint_files.append((checkpoint_file, self._last_checkpoint_time, max_unsynced_id))
            logger.info(f"æ£€æŸ¥ç‚¹å·²åˆ›å»º: {checkpoint_file} (æœ€å¤§æœªåŒæ­¥æ–‡ä»¶ID: {max_unsynced_id})")
            
            # æ¸…ç†è¿‡æœŸçš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            await self._cleanup_old_checkpoints()

        except Exception as e:
            logger.error(f"åˆ›å»ºæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    async def _get_max_unsynced_file_id(self) -> int:
        """è·å–å½“å‰æœ€å¤§æœªåŒæ­¥æ–‡ä»¶IDï¼ˆä»…å½“å‰å¤‡ä»½é›†ï¼‰"""
        try:
            async with self.memory_db.execute("""
                SELECT MAX(id) FROM backup_files WHERE backup_set_id = ? AND synced_to_opengauss = FALSE
            """, (self.backup_set_db_id,)) as cursor:
                result = await cursor.fetchone()
                return result[0] if result and result[0] is not None else 0
        except Exception as e:
            logger.debug(f"è·å–æœ€å¤§æœªåŒæ­¥æ–‡ä»¶IDå¤±è´¥: {e}")
            return 0
    
    async def _cleanup_old_checkpoints_on_startup(self):
        """å¯åŠ¨æ—¶æ¸…ç†æ‰€æœ‰è¿‡æœŸçš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if not self.enable_checkpoint:
            return  # æ£€æŸ¥ç‚¹åŠŸèƒ½å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        
        try:
            import os
            current_time = time.time()
            retention_seconds = self.checkpoint_retention_hours * 3600
            
            # æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•ä¸­çš„æ‰€æœ‰è¿‡æœŸæ–‡ä»¶
            if self.checkpoint_dir and self.checkpoint_dir.exists():
                import glob
                pattern = str(self.checkpoint_dir / 'tmp*.sql')
                cleaned_count = 0
                for old_file in glob.glob(pattern):
                    try:
                        file_stat = os.stat(old_file)
                        file_age = current_time - file_stat.st_mtime
                        if file_age > retention_seconds:
                            os.remove(old_file)
                            cleaned_count += 1
                            logger.debug(f"å¯åŠ¨æ—¶å·²åˆ é™¤è¿‡æœŸæ£€æŸ¥ç‚¹æ–‡ä»¶: {old_file}")
                    except Exception as e:
                        logger.debug(f"æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶æ—¶å‡ºé”™ï¼ˆå¿½ç•¥ï¼‰: {old_file}, {e}")
                
                if cleaned_count > 0:
                    logger.info(f"å¯åŠ¨æ—¶å·²æ¸…ç† {cleaned_count} ä¸ªè¿‡æœŸæ£€æŸ¥ç‚¹æ–‡ä»¶")
            else:
                # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
                
        except Exception as e:
            logger.warning(f"å¯åŠ¨æ—¶æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _cleanup_synced_checkpoints(self):
        """æ¸…ç†å·²å®Œå…¨åŒæ­¥åˆ°openGaussçš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if not self.enable_checkpoint:
            return  # æ£€æŸ¥ç‚¹åŠŸèƒ½å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        try:
            import os
            # è·å–å½“å‰å·²åŒæ­¥çš„æœ€å¤§æ–‡ä»¶ID
            async with self.memory_db.execute("""
                SELECT MAX(id) FROM backup_files WHERE synced_to_opengauss = TRUE
            """) as cursor:
                result = await cursor.fetchone()
                max_synced_id = result[0] if result and result[0] is not None else 0
            
            if max_synced_id <= 0:
                return  # è¿˜æ²¡æœ‰åŒæ­¥ä»»ä½•æ–‡ä»¶
            
            # æ¸…ç†æ‰€æœ‰å·²å®Œå…¨åŒæ­¥çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            # å¦‚æœæ£€æŸ¥ç‚¹åˆ›å»ºæ—¶çš„æœ€å¤§æœªåŒæ­¥æ–‡ä»¶ID <= å½“å‰å·²åŒæ­¥çš„æœ€å¤§æ–‡ä»¶IDï¼Œè¯´æ˜è¯¥æ£€æŸ¥ç‚¹çš„æ‰€æœ‰æ•°æ®éƒ½å·²åŒæ­¥
            files_to_remove = []
            for checkpoint_info in self._checkpoint_files[:]:
                if len(checkpoint_info) >= 3:
                    checkpoint_file, create_time, max_unsynced_id = checkpoint_info
                    # å¦‚æœæ£€æŸ¥ç‚¹åˆ›å»ºæ—¶çš„æœ€å¤§æœªåŒæ­¥æ–‡ä»¶ID <= å½“å‰å·²åŒæ­¥çš„æœ€å¤§æ–‡ä»¶IDï¼Œè¯´æ˜è¯¥æ£€æŸ¥ç‚¹çš„æ‰€æœ‰æ•°æ®éƒ½å·²åŒæ­¥
                    if max_unsynced_id <= max_synced_id:
                        try:
                            if os.path.exists(checkpoint_file):
                                os.remove(checkpoint_file)
                                logger.info(f"æ£€æŸ¥ç‚¹æ–‡ä»¶å·²å®Œå…¨åŒæ­¥åˆ°openGaussï¼Œå·²åˆ é™¤: {checkpoint_file} (æ£€æŸ¥ç‚¹æœ€å¤§æœªåŒæ­¥ID: {max_unsynced_id}, å½“å‰å·²åŒæ­¥æœ€å¤§ID: {max_synced_id})")
                            files_to_remove.append(checkpoint_info)
                        except Exception as e:
                            logger.warning(f"åˆ é™¤å·²åŒæ­¥çš„æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {checkpoint_file}, é”™è¯¯: {e}")
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼š(æ–‡ä»¶è·¯å¾„, åˆ›å»ºæ—¶é—´)
                    checkpoint_file, create_time = checkpoint_info
                    # æ—§æ ¼å¼çš„æ£€æŸ¥ç‚¹æ–‡ä»¶æ— æ³•åˆ¤æ–­æ˜¯å¦å·²å®Œå…¨åŒæ­¥ï¼Œè·³è¿‡
                    pass
            
            # ä»åˆ—è¡¨ä¸­ç§»é™¤å·²åˆ é™¤çš„æ–‡ä»¶
            for item in files_to_remove:
                if item in self._checkpoint_files:
                    self._checkpoint_files.remove(item)
                    
        except Exception as e:
            logger.warning(f"æ¸…ç†å·²åŒæ­¥çš„æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _cleanup_old_checkpoints(self):
        """æ¸…ç†è¿‡æœŸçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆå®šæœŸè°ƒç”¨ï¼‰"""
        if not self.enable_checkpoint:
            return  # æ£€æŸ¥ç‚¹åŠŸèƒ½å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        try:
            import os
            current_time = time.time()
            retention_seconds = self.checkpoint_retention_hours * 3600
            
            # æ¸…ç†è®°å½•åˆ—è¡¨ä¸­çš„è¿‡æœŸæ–‡ä»¶ï¼ˆä»…æ¸…ç†æœªåŒæ­¥çš„è¿‡æœŸæ–‡ä»¶ï¼‰
            files_to_remove = []
            for checkpoint_info in self._checkpoint_files[:]:
                if len(checkpoint_info) >= 3:
                    checkpoint_file, create_time, max_unsynced_id = checkpoint_info
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼š(æ–‡ä»¶è·¯å¾„, åˆ›å»ºæ—¶é—´)
                    checkpoint_file, create_time = checkpoint_info
                    max_unsynced_id = None
                
                if current_time - create_time > retention_seconds:
                    try:
                        if os.path.exists(checkpoint_file):
                            os.remove(checkpoint_file)
                            logger.debug(f"å·²åˆ é™¤è¿‡æœŸæ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_file}")
                        files_to_remove.append(checkpoint_info)
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {checkpoint_file}, é”™è¯¯: {e}")
            
            # ä»åˆ—è¡¨ä¸­ç§»é™¤å·²åˆ é™¤çš„æ–‡ä»¶
            for item in files_to_remove:
                if item in self._checkpoint_files:
                    self._checkpoint_files.remove(item)
            
            # åŒæ—¶æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•ä¸­å¯èƒ½é—ç•™çš„è¿‡æœŸæ–‡ä»¶ï¼ˆé€šè¿‡æ–‡ä»¶åæ¨¡å¼åŒ¹é…ï¼‰
            try:
                import glob
                if self.checkpoint_dir.exists():
                    pattern = str(self.checkpoint_dir / 'tmp*.sql')
                    for old_file in glob.glob(pattern):
                        try:
                            # å¦‚æœæ–‡ä»¶ä¸åœ¨è®°å½•åˆ—è¡¨ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                            file_in_list = any(old_file == (cf[0] if isinstance(cf, tuple) else cf) for cf in self._checkpoint_files)
                            if not file_in_list:
                                file_stat = os.stat(old_file)
                                file_age = current_time - file_stat.st_mtime
                                if file_age > retention_seconds:
                                    os.remove(old_file)
                                    logger.debug(f"å·²åˆ é™¤æ£€æŸ¥ç‚¹ç›®å½•ä¸­çš„è¿‡æœŸæ–‡ä»¶: {old_file}")
                        except Exception as e:
                            logger.debug(f"æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶æ—¶å‡ºé”™ï¼ˆå¿½ç•¥ï¼‰: {old_file}, {e}")
            except Exception as e:
                logger.debug(f"æ¸…ç†æ£€æŸ¥ç‚¹ç›®å½•æ–‡ä»¶å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")
                
        except Exception as e:
            logger.warning(f"æ¸…ç†è¿‡æœŸæ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _cleanup_all_checkpoints(self):
        """æ¸…ç†æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆåœæ­¢æ—¶è°ƒç”¨ï¼‰"""
        if not self.enable_checkpoint:
            return  # æ£€æŸ¥ç‚¹åŠŸèƒ½å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        try:
            import os
            for checkpoint_info in self._checkpoint_files[:]:
                # å…¼å®¹æ–°æ—§æ ¼å¼
                if len(checkpoint_info) >= 3:
                    checkpoint_file = checkpoint_info[0]
                else:
                    checkpoint_file = checkpoint_info[0]
                
                try:
                    if os.path.exists(checkpoint_file):
                        os.remove(checkpoint_file)
                        logger.debug(f"å·²åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_file}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {checkpoint_file}, é”™è¯¯: {e}")
            self._checkpoint_files.clear()
        except Exception as e:
            logger.warning(f"æ¸…ç†æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {e}")

    async def force_sync(self):
        """å¼ºåˆ¶åŒæ­¥æ‰€æœ‰å¾…åŒæ­¥æ–‡ä»¶"""
        logger.info("å¼ºåˆ¶åŒæ­¥æ‰€æœ‰å¾…åŒæ­¥æ–‡ä»¶")
        await self._sync_to_opengauss("force_sync")

    async def stop(self):
        """åœæ­¢å†…å­˜æ•°æ®åº“å†™å…¥å™¨"""
        logger.info("åœæ­¢å†…å­˜æ•°æ®åº“å†™å…¥å™¨")

        # åœæ­¢åŒæ­¥ä»»åŠ¡
        if self._sync_task:
            self._sync_task.cancel()
            try:
                await self._sync_task
            except asyncio.CancelledError:
                pass

        # å¦‚æœæ£€æŸ¥ç‚¹ä»»åŠ¡è¿˜åœ¨è¿è¡Œï¼Œç­‰å¾…å®ƒå®Œæˆï¼ˆä»…åœ¨å¯ç”¨æ£€æŸ¥ç‚¹æ—¶ï¼‰
        if self.enable_checkpoint and self._checkpoint_task:
            self._checkpoint_task.cancel()
            try:
                await self._checkpoint_task
            except asyncio.CancelledError:
                pass

        # æœ€åä¸€æ¬¡åŒæ­¥
        if self.memory_db:
            try:
                await self.force_sync()
                # åˆ›å»ºæœ€ç»ˆæ£€æŸ¥ç‚¹ï¼ˆä»…åœ¨å¯ç”¨æ£€æŸ¥ç‚¹æ—¶ï¼‰
                if self.enable_checkpoint:
                    await self._create_checkpoint()
            except Exception as e:
                logger.error(f"æœ€ç»ˆåŒæ­¥å¤±è´¥: {e}")
        
        # æ¸…ç†æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆåœæ­¢æ—¶ï¼Œä»…åœ¨å¯ç”¨æ£€æŸ¥ç‚¹æ—¶ï¼‰
        if self.enable_checkpoint:
            await self._cleanup_all_checkpoints()

        # å…³é—­æ•°æ®åº“è¿æ¥
        if self.memory_db:
            await self.memory_db.close()

    async def clear_database(self):
        """æ¸…ç©ºå†…å­˜æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ•°æ®ï¼ˆä»…å½“å‰å¤‡ä»½é›†ï¼‰"""
        if not self.memory_db:
            logger.warning("å†…å­˜æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ¸…ç©º")
            return
        
        try:
            # åˆ é™¤å½“å‰å¤‡ä»½é›†çš„æ‰€æœ‰æ–‡ä»¶è®°å½•
            async with self.memory_db.execute(
                "DELETE FROM backup_files WHERE backup_set_id = ?",
                (self.backup_set_db_id,)
            ) as cursor:
                deleted_count = cursor.rowcount
            
            await self.memory_db.commit()
            
            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self._stats = {
                'total_files': 0,
                'synced_files': 0,
                'sync_batches': 0,
                'total_time': 0,
                'sync_time': 0,
                'memory_usage': 0
            }
            
            logger.info(f"å·²æ¸…ç©ºå†…å­˜æ•°æ®åº“ï¼ˆå¤‡ä»½é›†ID: {self.backup_set_db_id}ï¼‰ï¼Œåˆ é™¤äº† {deleted_count} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºå†…å­˜æ•°æ®åº“å¤±è´¥: {e}", exc_info=True)
            raise

    async def check_database_schema(self):
        """æ£€æŸ¥å†…å­˜æ•°æ®åº“çš„å­—æ®µè®¾ç½®"""
        if not self.memory_db:
            logger.warning("å†…å­˜æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ£€æŸ¥")
            return
        
        try:
            # è·å–è¡¨ç»“æ„
            async with self.memory_db.execute("PRAGMA table_info(backup_files)") as cursor:
                columns = await cursor.fetchall()
            
            logger.info("========== å†…å­˜æ•°æ®åº“å­—æ®µæ£€æŸ¥ ==========")
            logger.info(f"è¡¨å: backup_files")
            logger.info(f"å­—æ®µæ•°é‡: {len(columns)}")
            logger.info("å­—æ®µåˆ—è¡¨:")
            for col in columns:
                col_id, col_name, col_type, not_null, default_val, pk = col
                logger.info(f"  [{col_id}] {col_name}: {col_type} (NOT NULL: {not_null}, DEFAULT: {default_val}, PK: {pk})")
            
            # æ£€æŸ¥å…³é”®å­—æ®µçš„é»˜è®¤å€¼
            synced_col = next((c for c in columns if c[1] == 'synced_to_opengauss'), None)
            if synced_col:
                logger.info(f"synced_to_opengauss å­—æ®µ: ç±»å‹={synced_col[2]}, é»˜è®¤å€¼={synced_col[4]}")
            else:
                logger.warning("æœªæ‰¾åˆ° synced_to_opengauss å­—æ®µï¼")
            
            # æ£€æŸ¥å½“å‰æ•°æ®çŠ¶æ€
            async with self.memory_db.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN synced_to_opengauss = TRUE THEN 1 END) as synced,
                    COUNT(CASE WHEN synced_to_opengauss = FALSE THEN 1 END) as pending,
                    COUNT(CASE WHEN synced_to_opengauss IS NULL THEN 1 END) as null_synced
                FROM backup_files
                WHERE backup_set_id = ?
            """, (self.backup_set_db_id,)) as cursor:
                result = await cursor.fetchone()
                if result:
                    total, synced, pending, null_synced = result
                    logger.info(f"å½“å‰æ•°æ®çŠ¶æ€ï¼ˆå¤‡ä»½é›†ID: {self.backup_set_db_id}ï¼‰:")
                    logger.info(f"  æ€»æ–‡ä»¶æ•°: {total}")
                    logger.info(f"  å·²åŒæ­¥: {synced}")
                    logger.info(f"  å¾…åŒæ­¥: {pending}")
                    logger.info(f"  synced_to_opengauss ä¸º NULL: {null_synced}")
                    if null_synced > 0:
                        logger.warning(f"âš ï¸ å‘ç° {null_synced} ä¸ªæ–‡ä»¶çš„ synced_to_opengauss å­—æ®µä¸º NULLï¼Œè¿™å¯èƒ½å¯¼è‡´åŒæ­¥é—®é¢˜ï¼")
            
            logger.info("=========================================")
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å†…å­˜æ•°æ®åº“å­—æ®µè®¾ç½®å¤±è´¥: {e}", exc_info=True)
            raise

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self._stats.copy()
        stats['memory_usage'] = self._stats['total_files'] * 2  # ä¼°ç®—å†…å­˜ä½¿ç”¨(KB)
        stats['pending_sync'] = self._stats['total_files'] - self._stats['synced_files']
        stats['sync_progress'] = (self._stats['synced_files'] / max(1, self._stats['total_files'])) * 100

        return stats

    async def get_sync_status(self) -> Dict:
        """è·å–åŒæ­¥çŠ¶æ€è¯¦æƒ…"""
        if not self.memory_db:
            return {'status': 'not_initialized'}

        async with self.memory_db.execute("""
            SELECT
                COUNT(*) as total_files,
                COUNT(CASE WHEN synced_to_opengauss = TRUE THEN 1 END) as synced_files,
                COUNT(CASE WHEN synced_to_opengauss = FALSE AND sync_error IS NOT NULL THEN 1 END) as error_files,
                COUNT(CASE WHEN synced_to_opengauss = FALSE THEN 1 END) as pending_files,
                SUM(file_size) as total_size
            FROM backup_files
        """) as cursor:
            result = await cursor.fetchone()

            return {
                'total_files': result[0],
                'synced_files': result[1],
                'error_files': result[2],
                'pending_files': result[3],
                'total_size': result[4] or 0,
                'sync_progress': (result[1] / max(1, result[0])) * 100,
                'is_syncing': self._is_syncing,
                'last_sync_time': self._last_sync_time,
                'last_checkpoint_time': self._last_checkpoint_time
            }